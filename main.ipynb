{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from sqlalchemy import create_engine, inspect, text\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, adjusted_rand_score, normalized_mutual_info_score, silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Configuration class\n",
    "class Config:\n",
    "    # Database configuration\n",
    "    DATABASE = {\n",
    "        'database': os.getenv('DB_NAME', 'maude'),\n",
    "        'user': os.getenv('DB_USER', 'postgres'),\n",
    "        'password': os.getenv('DB_PASSWORD', '12345687'),\n",
    "        'host': os.getenv('DB_HOST', '192.168.8.167'),\n",
    "        'port': os.getenv('DB_PORT', '5432')\n",
    "    }\n",
    "    SCHEMA = 'maude'\n",
    "    SAMPLE_SIZE = 3  # Number of sample data per table\n",
    "    OUTPUT_DIR = os.getenv('OUTPUT_DIR', 'maude_schema_analysis')\n",
    "    COMBINED_OUTPUT_FILE = os.getenv('COMBINED_OUTPUT_FILE', 'maude_schema_combined.txt')\n",
    "    DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')\n",
    "    DEEPSEEK_BASE_URL = os.getenv('DEEPSEEK_BASE_URL', 'https://api.deepseek.com/v1')\n",
    "    OUTPUT_ANALYSIS_FILE = os.getenv('OUTPUT_ANALYSIS_FILE', 'maude_schema_analysis.json')\n",
    "    RETRY_LIMIT = int(os.getenv('RETRY_LIMIT', 2))  # Number of retries for failed API calls\n",
    "    OVERWRITE_EXISTING = os.getenv('OVERWRITE_EXISTING', 'True').lower() in ['true', '1', 't']  # Whether to overwrite existing fields\n",
    "    MAX_TOKENS = int(os.getenv('MAX_TOKENS', 100000))  # Maximum token limit, adjust based on actual API\n",
    "    # Similarity thresholds list\n",
    "    SIMILARITY_THRESHOLDS = [0.7, 0.8, 0.9]  # Similarity thresholds for merging\n",
    "    PREFILTER_JACCARD_THRESHOLD = 0.1  # Pre-filtering Jaccard similarity threshold\n",
    "    DISTANCE_THRESHOLDS = [1.0]  # Distance thresholds for hierarchical clustering (simplified to a single value)\n",
    "    GLOBAL_CONTEXT_FILE = os.getenv('GLOBAL_CONTEXT_FILE', 'context.txt')  # Path to global context file\n",
    "    USE_HIERARCHICAL_CLUSTERING = os.getenv('USE_HIERARCHICAL_CLUSTERING', 'True').lower() in ['true', '1', 't']\n",
    "    ENABLE_SIMILARITY_CALCULATION = os.getenv('ENABLE_SIMILARITY_CALCULATION', 'True').lower() in ['true', '1', 't']  # Whether to enable similarity calculation and merging\n",
    "    # Path to manual grouping standard result file (expert-based)\n",
    "    MANUAL_GROUPING_FILE = os.getenv('MANUAL_GROUPING_FILE', 'manual_grouping.json')\n",
    "    # Path to save evaluation results\n",
    "    EVALUATION_RESULTS_FILE = os.getenv('EVALUATION_RESULTS_FILE', 'evaluation_results.csv')\n",
    "    # Clustering methods and parameters (simplified to K-Means, Hierarchical, DBSCAN)\n",
    "    CLUSTERING_METHODS = {\n",
    "        'kmeans_manual': {\n",
    "            'params': [3, 4, 5, 6, 7],\n",
    "            'description': 'K-Means clustering with manually specified cluster counts'\n",
    "        },\n",
    "        'kmeans_auto': {\n",
    "            'params': [None],  # None indicates automatic detection\n",
    "            'description': 'K-Means clustering with automatic detection of optimal cluster count'\n",
    "        },\n",
    "        'hierarchical': {\n",
    "            'params': [0.8, 1.0, 1.2],\n",
    "            'description': 'Hierarchical clustering with varying distance thresholds'\n",
    "        },\n",
    "        'dbscan_manual': {\n",
    "            'params': [(0.5, 5), (0.6, 5), (0.7, 5)],\n",
    "            'description': 'DBSCAN clustering with varying ε values and min_samples=5'\n",
    "        },\n",
    "        'dbscan_auto': {\n",
    "            'params': [None],  # None indicates automatic parameter selection\n",
    "            'description': 'DBSCAN clustering with automatic selection of ε'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Cache file path\n",
    "    CACHE_FILE = os.getenv('CACHE_FILE', 'similarity_cache.json')\n",
    "    # Whether to enable PCA dimensionality reduction\n",
    "    ENABLE_PCA = os.getenv('ENABLE_PCA', 'True').lower() in ['true', '1', 't']\n",
    "    # Feature extraction methods\n",
    "    FEATURE_EXTRACTION_METHODS = ['tfidf', 'sentence_transformer']  # 'tfidf' or 'sentence_transformer'\n",
    "\n",
    "# Create output directory\n",
    "def create_output_directory(output_dir: str):\n",
    "    \"\"\"Create the output directory if it does not exist.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        logging.info(f\"Created output directory: {output_dir}\")\n",
    "    else:\n",
    "        logging.info(f\"Output directory already exists: {output_dir}\")\n",
    "\n",
    "# Create and return a SQLAlchemy engine\n",
    "def get_database_engine(db_config: dict):\n",
    "    \"\"\"Create and return a SQLAlchemy engine.\"\"\"\n",
    "    db_uri = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "    try:\n",
    "        engine = create_engine(db_uri)\n",
    "        # Test connection\n",
    "        with engine.connect() as connection:\n",
    "            connection.execute(text(\"SELECT 1\"))\n",
    "        logging.info(\"Successfully connected to the database.\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to connect to the database: {e}\")\n",
    "        raise\n",
    "\n",
    "# Get all table names in the specified schema\n",
    "def get_all_tables(engine, schema: str) -> list:\n",
    "    \"\"\"Retrieve all table names in the specified schema.\"\"\"\n",
    "    inspector = inspect(engine)\n",
    "    tables = inspector.get_table_names(schema=schema)\n",
    "    logging.info(f\"Found {len(tables)} tables in schema '{schema}'.\")\n",
    "    return tables\n",
    "\n",
    "# Get the structure information of a specified table\n",
    "def get_table_structure(inspector, table_name: str, schema: str) -> list:\n",
    "    \"\"\"Retrieve the structure information of a specified table.\"\"\"\n",
    "    try:\n",
    "        columns = inspector.get_columns(table_name, schema=schema)\n",
    "        column_info = [{\n",
    "            'name': column['name'],\n",
    "            'type': str(column['type']),\n",
    "            'nullable': column['nullable'],\n",
    "            'default': str(column['default']) if column['default'] else None\n",
    "        } for column in columns]\n",
    "        return column_info\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving table structure ({table_name}): {e}\")\n",
    "        return []\n",
    "\n",
    "# Get sample data from a specified table\n",
    "def get_sample_data(engine, table_name: str, schema: str, sample_size: int) -> list:\n",
    "    \"\"\"Retrieve sample data from a specified table.\"\"\"\n",
    "    try:\n",
    "        query = f'SELECT * FROM \"{schema}\".\"{table_name}\" LIMIT {sample_size};'\n",
    "        df = pd.read_sql(query, engine)\n",
    "        sample_data = df.to_dict(orient='records')\n",
    "        return sample_data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving sample data ({table_name}): {e}\")\n",
    "        return []\n",
    "\n",
    "# Anonymize sample data by redacting sensitive fields\n",
    "def anonymize_sample_data(sample_data: List[Dict[str, any]]) -> List[Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    Anonymize sample data by redacting or obfuscating sensitive fields.\n",
    "    Customize as needed.\n",
    "    \"\"\"\n",
    "    sensitive_fields = ['patient_id', 'first_name', 'last_name', 'date_of_birth', 'name', 'address']\n",
    "    anonymized_data = []\n",
    "    for record in sample_data:\n",
    "        anonymized_record = {}\n",
    "        for key, value in record.items():\n",
    "            if key in sensitive_fields:\n",
    "                anonymized_record[key] = \"REDACTED\"\n",
    "            else:\n",
    "                anonymized_record[key] = value\n",
    "        anonymized_data.append(anonymized_record)\n",
    "    return anonymized_data\n",
    "\n",
    "# Save table structure and sample data to a JSON file and validate JSON format\n",
    "def save_table_info(output_dir: str, table_name: str, structure: list, samples: list):\n",
    "    \"\"\"Save table structure and sample data to a JSON file and validate JSON format.\"\"\"\n",
    "    # Anonymize sample data\n",
    "    anonymized_samples = anonymize_sample_data(samples)\n",
    "    \n",
    "    data = {\n",
    "        'table_name': table_name,\n",
    "        'structure': structure,\n",
    "        'sample_data': anonymized_samples\n",
    "    }\n",
    "    file_path = os.path.join(output_dir, f\"{table_name}.json\")\n",
    "    try:\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "        # Validate JSON file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            json.load(f)  # Attempt to reload to ensure JSON is valid\n",
    "        \n",
    "        logging.info(f\"Saved and validated data for table '{table_name}' to '{file_path}'.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        logging.error(f\"Invalid JSON generated when saving table '{table_name}': {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving table data ({table_name}): {e}\")\n",
    "\n",
    "# Generate table description including fields and sample data summary for similarity analysis\n",
    "def generate_table_description(table_name: str, structure: list, samples: list) -> str:\n",
    "    \"\"\"\n",
    "    Generate a description of the table, including fields and a summary of sample data, for similarity analysis.\n",
    "    \"\"\"\n",
    "    description = f\"Table '{table_name}' has the following fields: \"\n",
    "    fields = [f\"{column['name']} ({column['type']})\" for column in structure]\n",
    "    description += \", \".join(fields) + \". \"\n",
    "    \n",
    "    if samples:\n",
    "        description += \"Sample data includes: \"\n",
    "        sample_summaries = []\n",
    "        for column in structure:\n",
    "            col_name = column['name']\n",
    "            # Extract the first 3 sample values for each column\n",
    "            sample_values = [str(record[col_name]) for record in samples[:3] if col_name in record]\n",
    "            if sample_values:\n",
    "                sample_summaries.append(f\"{col_name} values like {', '.join(sample_values)}\")\n",
    "        description += \"; \".join(sample_summaries) + \".\"\n",
    "    \n",
    "    return description\n",
    "\n",
    "# Retrieve descriptions for all tables\n",
    "def get_all_table_descriptions(config: Config) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Retrieve descriptions for all tables, including sample data summaries.\n",
    "    Returns a dictionary with table names as keys and descriptions as values.\n",
    "    \"\"\"\n",
    "    descriptions = {}\n",
    "    json_files = [f for f in os.listdir(config.OUTPUT_DIR) if f.endswith('.json')]\n",
    "    for json_file in json_files:\n",
    "        file_path = os.path.join(config.OUTPUT_DIR, json_file)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            table_name = data.get('table_name', 'Unknown Table')\n",
    "            structure = data.get('structure', [])\n",
    "            samples = data.get('sample_data', [])\n",
    "            description = generate_table_description(table_name, structure, samples)\n",
    "            descriptions[table_name] = description\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.error(f\"JSON parsing error in file '{json_file}': {e}\")\n",
    "            # Optionally, delete the problematic file\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                logging.info(f\"Deleted problematic file '{json_file}'.\")\n",
    "            except Exception as remove_error:\n",
    "                logging.error(f\"Error deleting file '{json_file}': {remove_error}\")\n",
    "            continue  # Skip the problematic file\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing file '{json_file}': {e}\")\n",
    "            continue\n",
    "    return descriptions\n",
    "\n",
    "# Call the DeepSeek API\n",
    "def call_deepseek_api(prompt: str, config: Config) -> str:\n",
    "    \"\"\"\n",
    "    Call the DeepSeek API and return the response content.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {config.DEEPSEEK_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-chat\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    for attempt in range(1, config.RETRY_LIMIT + 1):\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = requests.post(\n",
    "                f\"{config.DEEPSEEK_BASE_URL}/chat/completions\",\n",
    "                headers=headers,\n",
    "                data=json.dumps(payload),\n",
    "                timeout=300  # Increase timeout to handle long texts\n",
    "            )\n",
    "            elapsed_time = time.time() - start_time\n",
    "            logging.info(f\"API call attempt {attempt}, took {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                content = result.get('choices', [{}])[0].get('message', {}).get('content', '').strip()\n",
    "                \n",
    "                # Check JSON integrity\n",
    "                if content.count('{') == content.count('}'):\n",
    "                    return content\n",
    "                else:\n",
    "                    logging.error(f\"Incomplete JSON returned, retrying API call. Attempt {attempt}.\")\n",
    "            else:\n",
    "                logging.error(f\"DeepSeek API error {response.status_code}: {response.text}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"API call exception on attempt {attempt}: {e}\")\n",
    "\n",
    "        if attempt < config.RETRY_LIMIT:\n",
    "            logging.info(f\"Waiting before retrying... (Attempt {attempt})\")\n",
    "\n",
    "    logging.error(\"All API call attempts failed.\")\n",
    "    return \"\"\n",
    "\n",
    "# Extract JSON object from text\n",
    "def extract_json_response(text: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Extract a JSON object from the given text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove code block markers\n",
    "        text = re.sub(r'```json', '', text)\n",
    "        text = re.sub(r'```', '', text)\n",
    "        \n",
    "        # Find the complete JSON object\n",
    "        json_match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "        if not json_match:\n",
    "            logging.error(\"No JSON object found in the response.\")\n",
    "            return None\n",
    "        json_str = json_match.group()\n",
    "        json_obj = json.loads(json_str)\n",
    "        \n",
    "        return json_obj\n",
    "    except json.JSONDecodeError as e:\n",
    "        logging.error(f\"JSON decoding error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting JSON: {e}\")\n",
    "        return None\n",
    "\n",
    "# Generate TF-IDF feature vectors\n",
    "def generate_tfidf_features(descriptions: Dict[str, str]) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Generate TF-IDF feature vectors for table descriptions.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    table_names = list(descriptions.keys())\n",
    "    corpus = [descriptions[table] for table in table_names]\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    return tfidf_matrix.toarray(), table_names\n",
    "\n",
    "# Generate SentenceTransformer embeddings\n",
    "def generate_sentence_embeddings(descriptions: Dict[str, str], model_name: str = 'all-MiniLM-L6-v2') -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Generate SentenceTransformer embeddings for table descriptions.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    table_names = list(descriptions.keys())\n",
    "    corpus = [descriptions[table] for table in table_names]\n",
    "    embeddings = model.encode(corpus, show_progress_bar=True)\n",
    "    return embeddings, table_names\n",
    "\n",
    "# Perform PCA dimensionality reduction\n",
    "def reduce_dimensionality(feature_matrix: np.ndarray, n_components: int = 100) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform PCA dimensionality reduction on the feature matrix.\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    reduced_matrix = pca.fit_transform(feature_matrix)\n",
    "    logging.info(f\"Dimensionality reduced to: {reduced_matrix.shape}\")\n",
    "    return reduced_matrix\n",
    "\n",
    "# Hierarchical Clustering\n",
    "def cluster_tables_hierarchical(tfidf_matrix: np.ndarray, distance_threshold: float, table_names: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Perform hierarchical clustering on tables.\n",
    "    \"\"\"\n",
    "    # Compute distance matrix\n",
    "    distance_matrix = cosine_distances(tfidf_matrix)\n",
    "    \n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=None,\n",
    "        metric='precomputed',\n",
    "        linkage='average',\n",
    "        distance_threshold=distance_threshold\n",
    "    )\n",
    "    clustering.fit(distance_matrix)\n",
    "    \n",
    "    labels = clustering.labels_\n",
    "    clusters = {}\n",
    "    for table, label in zip(table_names, labels):\n",
    "        clusters.setdefault(label, []).append(table)\n",
    "    return list(clusters.values())\n",
    "\n",
    "# K-Means Clustering\n",
    "def cluster_tables_kmeans(tfidf_matrix: np.ndarray, n_clusters: Optional[int], table_names: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Perform K-Means clustering on tables.\n",
    "    If n_clusters is None, automatically detect the optimal number of clusters.\n",
    "    \"\"\"\n",
    "    if n_clusters is None:\n",
    "        logging.info(\"Automatically detecting the number of K-Means clusters.\")\n",
    "        optimal_k = find_optimal_kmeans_clusters(tfidf_matrix, 2, 20)\n",
    "        if optimal_k is None:\n",
    "            logging.error(\"Unable to determine the optimal number of K-Means clusters, using default K=5.\")\n",
    "            n_clusters = 5\n",
    "        else:\n",
    "            n_clusters = optimal_k\n",
    "            logging.info(f\"Optimal number of K-Means clusters detected: {n_clusters}.\")\n",
    "    clustering = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clustering.fit(tfidf_matrix)\n",
    "    labels = clustering.labels_\n",
    "    clusters = {}\n",
    "    for table, label in zip(table_names, labels):\n",
    "        clusters.setdefault(label, []).append(table)\n",
    "    return list(clusters.values())\n",
    "\n",
    "# DBSCAN Clustering\n",
    "def cluster_tables_dbscan(feature_matrix: np.ndarray, eps: Optional[float], min_samples: int, table_names: List[str], config: Config) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Perform DBSCAN clustering on tables.\n",
    "    If eps is None, automatically select parameters.\n",
    "    \"\"\"\n",
    "    if eps is None:\n",
    "        logging.info(\"Automatically selecting DBSCAN parameters.\")\n",
    "        eps = find_dbscan_eps(feature_matrix)\n",
    "        if eps is None:\n",
    "            logging.error(\"Unable to determine DBSCAN's eps value automatically, using default eps=0.5.\")\n",
    "            eps = 0.5\n",
    "        logging.info(f\"Automatically selected DBSCAN eps value: {eps}.\")\n",
    "    clustering = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')\n",
    "    labels = clustering.fit_predict(feature_matrix)\n",
    "    clusters = {}\n",
    "    for table, label in zip(table_names, labels):\n",
    "        if label == -1:\n",
    "            continue  # Ignore noise points\n",
    "        clusters.setdefault(label, []).append(table)\n",
    "    return list(clusters.values())\n",
    "\n",
    "# Automatically select DBSCAN's eps value using k-distance graph\n",
    "def find_dbscan_eps(feature_matrix: np.ndarray, k: int = 4) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Automatically select DBSCAN's eps value using the k-distance graph.\n",
    "    \n",
    "    Parameters:\n",
    "    - feature_matrix: Feature matrix of tables\n",
    "    - k: The k-th nearest neighbor\n",
    "    \n",
    "    Returns:\n",
    "    - Selected eps value, or None if unable to determine\n",
    "    \"\"\"\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "    try:\n",
    "        neighbors = NearestNeighbors(n_neighbors=k)\n",
    "        neighbors_fit = neighbors.fit(feature_matrix)\n",
    "        distances, indices = neighbors_fit.kneighbors(feature_matrix)\n",
    "        distances = np.sort(distances[:, k-1], axis=0)\n",
    "        # Plot the k-distance graph\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(distances)\n",
    "        plt.ylabel(f\"{k}-Distance\")\n",
    "        plt.xlabel(\"Sample Index\")\n",
    "        plt.title(f\"{k}-Distance Graph for Selecting DBSCAN eps\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Find the elbow point (knee)\n",
    "        # Use the difference method to find the largest second derivative\n",
    "        diff = np.diff(distances)\n",
    "        diff2 = np.diff(diff)\n",
    "        elbow_index = np.argmax(diff2) + 2  # +2 because diff2 is offset by two elements\n",
    "        eps = distances[elbow_index]\n",
    "        logging.info(f\"Selected DBSCAN eps value based on k-distance graph: {eps}.\")\n",
    "        return eps\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error automatically selecting DBSCAN eps value: {e}\")\n",
    "        return None\n",
    "\n",
    "# Automatically detect the optimal number of K-Means clusters using Silhouette Score\n",
    "def find_optimal_kmeans_clusters(tfidf_matrix: np.ndarray, min_k: int = 2, max_k: int = 20) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Automatically determine the optimal number of K-Means clusters using the Silhouette Score.\n",
    "    \n",
    "    Parameters:\n",
    "    - tfidf_matrix: TF-IDF feature matrix of table descriptions\n",
    "    - min_k: Minimum number of clusters\n",
    "    - max_k: Maximum number of clusters\n",
    "    \n",
    "    Returns:\n",
    "    - Optimal number of clusters, or None if unable to determine\n",
    "    \"\"\"\n",
    "    best_k = None\n",
    "    best_score = -1\n",
    "    for k in range(min_k, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(tfidf_matrix)\n",
    "        if len(set(labels)) == 1:\n",
    "            # Silhouette Score cannot be calculated\n",
    "            continue\n",
    "        score = silhouette_score(tfidf_matrix, labels)\n",
    "        logging.info(f\"K-Means Clusters: {k}, Silhouette Score: {score:.4f}\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_k = k\n",
    "    if best_k is not None:\n",
    "        logging.info(f\"Optimal number of K-Means clusters: {best_k}, Silhouette Score: {best_score:.4f}\")\n",
    "    else:\n",
    "        logging.warning(\"Unable to determine the optimal number of K-Means clusters.\")\n",
    "    return best_k\n",
    "\n",
    "# Load similarity cache\n",
    "def load_cache(cache_file: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Load the similarity cache.\n",
    "    \"\"\"\n",
    "    if os.path.exists(cache_file):\n",
    "        try:\n",
    "            with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "                cache = json.load(f)\n",
    "            logging.info(f\"Successfully loaded cache file '{cache_file}'.\")\n",
    "            return cache\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading cache file '{cache_file}': {e}\")\n",
    "            return {}\n",
    "    else:\n",
    "        logging.info(f\"Cache file '{cache_file}' does not exist, creating a new cache.\")\n",
    "        return {}\n",
    "\n",
    "# Save similarity cache\n",
    "def save_cache(cache_file: str, cache: Dict[str, float]):\n",
    "    \"\"\"\n",
    "    Save the similarity cache.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(cache, f, ensure_ascii=False, indent=4)\n",
    "        logging.info(f\"Cache saved to '{cache_file}'.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving cache to '{cache_file}': {e}\")\n",
    "\n",
    "# Compute similarity scores within clusters\n",
    "def compute_similarity_within_clusters(descriptions: Dict[str, str], config: Config, clusters: List[List[str]], max_workers: int = 3) -> Tuple[Dict[Tuple[str, str], float], int]:\n",
    "    \"\"\"\n",
    "    Compute similarity scores for table pairs within each cluster.\n",
    "    Returns a dictionary of similarity scores and the number of pairs that require similarity computation.\n",
    "    \"\"\"\n",
    "    similarity_scores = {}\n",
    "    total_pairs = 0\n",
    "    pairs_to_process = []\n",
    "    prefilter_threshold = config.PREFILTER_JACCARD_THRESHOLD  # Fixed threshold of 0.1\n",
    "\n",
    "    # Load cache\n",
    "    cache = load_cache(config.CACHE_FILE)\n",
    "\n",
    "    # Generate all necessary table pairs and apply pre-filtering\n",
    "    for cluster in clusters:\n",
    "        if len(cluster) < 2:\n",
    "            continue  # No need to compute similarity for single tables\n",
    "        n = len(cluster)\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                pair = tuple(sorted((cluster[i], cluster[j])))\n",
    "                # Pre-filtering: Jaccard similarity >= prefilter_threshold\n",
    "                fields1 = set([col.split('(')[0].strip() for col in descriptions[pair[0]].split(': ')[1].split(', ')])\n",
    "                fields2 = set([col.split('(')[0].strip() for col in descriptions[pair[1]].split(': ')[1].split(', ')])\n",
    "                intersection = fields1.intersection(fields2)\n",
    "                union = fields1.union(fields2)\n",
    "                jaccard = len(intersection) / len(union) if union else 0.0\n",
    "                total_pairs += 1  # Count total number of table pairs\n",
    "                if jaccard >= prefilter_threshold:\n",
    "                    pairs_to_process.append(pair)\n",
    "\n",
    "    logging.info(f\"Total table pairs: {total_pairs}, Pairs passing pre-filtering: {len(pairs_to_process)} (require similarity computation).\")\n",
    "\n",
    "    def compute_pair_similarity(pair: Tuple[str, str]) -> Tuple[Tuple[str, str], float]:\n",
    "        table1, table2 = pair\n",
    "        cache_key = f\"{table1}|{table2}\"\n",
    "        if cache_key in cache:\n",
    "            score = cache[cache_key]\n",
    "            logging.info(f\"Retrieved similarity score for tables '{table1}' and '{table2}' from cache: {score}\")\n",
    "            return pair, score\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Please evaluate the similarity between the following two database tables. Return a similarity score between 0 and 1, where 0 indicates completely dissimilar and 1 indicates identical similarity.\n",
    "\n",
    "Table 1 Description: {descriptions[table1]}\n",
    "Table 2 Description: {descriptions[table2]}\n",
    "\n",
    "Please return only a JSON object in the format similar to {{\"similarity_score\": 0.85}} without any additional content.\n",
    "\"\"\"\n",
    "        start_time = time.time()\n",
    "        response = call_deepseek_api(prompt, config)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logging.info(f\"API call for tables '{table1}' and '{table2}' took {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "        if response:\n",
    "            json_obj = extract_json_response(response)\n",
    "            if json_obj and \"similarity_score\" in json_obj:\n",
    "                score = float(json_obj[\"similarity_score\"])\n",
    "                logging.info(f\"Similarity score for tables '{table1}' and '{table2}': {score}\")\n",
    "                cache[cache_key] = score  # Update cache\n",
    "                return pair, score\n",
    "            else:\n",
    "                logging.warning(f\"Similarity score not found in response for tables '{table1}' and '{table2}'.\")\n",
    "                return pair, 0.0\n",
    "        else:\n",
    "            logging.warning(f\"No valid similarity score received for tables '{table1}' and '{table2}'.\")\n",
    "            return pair, 0.0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_pair = {executor.submit(compute_pair_similarity, pair): pair for pair in pairs_to_process}\n",
    "        for future in as_completed(future_to_pair):\n",
    "            pair, score = future.result()\n",
    "            similarity_scores[pair] = score\n",
    "\n",
    "    # Save updated cache\n",
    "    save_cache(config.CACHE_FILE, cache)\n",
    "\n",
    "    return similarity_scores, len(pairs_to_process)  # Return similarity scores and number of pairs processed\n",
    "\n",
    "# Merge tables based on similarity threshold\n",
    "def merge_tables_by_similarity(similarity_scores: Dict[Tuple[str, str], float], threshold: float, all_tables: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Merge tables based on the similarity threshold, ensuring all tables are included in the final clusters.\n",
    "    \n",
    "    Parameters:\n",
    "    - similarity_scores: Dictionary of table pairs and their similarity scores\n",
    "    - threshold: Similarity threshold for merging\n",
    "    - all_tables: List of all table names\n",
    "    \n",
    "    Returns:\n",
    "    - List of clusters, each cluster is a list of table names\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add all tables as nodes\n",
    "    G.add_nodes_from(all_tables)\n",
    "    \n",
    "    # Add edges for table pairs that meet the similarity threshold\n",
    "    for (table1, table2), score in similarity_scores.items():\n",
    "        if score >= threshold:\n",
    "            G.add_edge(table1, table2)\n",
    "    \n",
    "    # Get all connected components as clusters\n",
    "    clusters = list(nx.connected_components(G))\n",
    "    return [list(cluster) for cluster in clusters]\n",
    "\n",
    "# Load manual grouping results\n",
    "def load_manual_grouping(file_path: str, ai_grouping: Dict[str, int]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Load manual grouping results. If the file does not exist, generate an initial file using AI grouping results.\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                manual_grouping = json.load(f)\n",
    "            logging.info(f\"Successfully loaded manual grouping file '{file_path}'.\")\n",
    "            return manual_grouping\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading manual grouping file '{file_path}': {e}\")\n",
    "            return {}\n",
    "    else:\n",
    "        # If the file does not exist, generate an initial manual grouping file using AI results\n",
    "        try:\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(ai_grouping, f, indent=4, ensure_ascii=False)\n",
    "            logging.info(f\"Manual grouping file '{file_path}' did not exist and has been generated with AI grouping results. Please perform manual adjustments.\")\n",
    "            return ai_grouping\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating initial manual grouping file '{file_path}': {e}\")\n",
    "            return {}\n",
    "\n",
    "# Evaluate clustering results by comparing manual and AI groupings\n",
    "def evaluate_groupings(manual_grouping: Dict[str, int], ai_grouping: Dict[str, int]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compare manual grouping results with AI grouping results and calculate evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Convert groupings to lists\n",
    "    tables = list(manual_grouping.keys())\n",
    "    manual_labels = [manual_grouping[table] for table in tables]\n",
    "    ai_labels = [ai_grouping.get(table, -1) for table in tables]  # If AI did not group, mark as -1\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    ari = adjusted_rand_score(manual_labels, ai_labels)\n",
    "    nmi = normalized_mutual_info_score(manual_labels, ai_labels)\n",
    "\n",
    "    # Due to the nature of clustering labels, precision, recall, and f1 require special handling\n",
    "    from itertools import combinations\n",
    "    manual_pairs = set()\n",
    "    ai_pairs = set()\n",
    "    for (i, j) in combinations(range(len(tables)), 2):\n",
    "        if manual_labels[i] == manual_labels[j]:\n",
    "            manual_pairs.add((i, j))\n",
    "        if ai_labels[i] == ai_labels[j]:\n",
    "            ai_pairs.add((i, j))\n",
    "\n",
    "    tp = len(manual_pairs & ai_pairs)\n",
    "    fp = len(ai_pairs - manual_pairs)\n",
    "    fn = len(manual_pairs - ai_pairs)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    metrics = {\n",
    "        'adjusted_rand_index': ari,\n",
    "        'normalized_mutual_info': nmi,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Save AI grouping results to a CSV file\n",
    "def save_groupings_to_csv(ai_grouping: Dict[str, int], config: Config, similarity_threshold: Optional[float], clustering_method: str, clustering_param: float):\n",
    "    \"\"\"\n",
    "    Save AI grouping results to a CSV file, recording each group and the tables it contains.\n",
    "    \"\"\"\n",
    "    # Reverse the grouping mapping: group ID -> [table names]\n",
    "    group_to_tables = {}\n",
    "    for table, group_id in ai_grouping.items():\n",
    "        group_to_tables.setdefault(group_id, []).append(table)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Group ID': list(group_to_tables.keys()),\n",
    "        'Tables': [', '.join(tables) for tables in group_to_tables.values()]\n",
    "    })\n",
    "    \n",
    "    # Define CSV filename including parameter information\n",
    "    if similarity_threshold is not None:\n",
    "        csv_filename = f\"groupings_{clustering_method}_sim_{similarity_threshold}_param_{clustering_param}.csv\"\n",
    "    else:\n",
    "        csv_filename = f\"groupings_{clustering_method}_param_{clustering_param}.csv\"\n",
    "    csv_path = os.path.join(config.OUTPUT_DIR, csv_filename)\n",
    "    \n",
    "    try:\n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "        logging.info(f\"AI grouping results saved to '{csv_path}'.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving AI grouping results to CSV: {e}\")\n",
    "\n",
    "# Count the number of tokens in text using tiktoken library\n",
    "def count_tokens(text: str, encoding_name: str = \"gpt2\") -> int:\n",
    "    \"\"\"\n",
    "    Count the number of tokens in the text using the tiktoken library.\n",
    "    Defaults to GPT-2 encoding, compatible with OpenAI's GPT models.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.get_encoding(encoding_name)\n",
    "        tokens = encoding.encode(text)\n",
    "        return len(tokens)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error counting tokens: {e}\")\n",
    "        return len(text)  # Fallback to character count\n",
    "\n",
    "# Split long text into multiple chunks\n",
    "def split_text_into_chunks(text: str, max_tokens: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split long text into multiple chunks, each with a token count not exceeding max_tokens.\n",
    "    Split by table to avoid breaking in the middle of table information.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting to split long text into multiple chunks...\")\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_tokens = 0\n",
    "\n",
    "    # Split by two newlines, assuming each table's information is separated by two newlines\n",
    "    tables = text.split('\\n\\n')\n",
    "    for table in tables:\n",
    "        table_length = count_tokens(table) + count_tokens(\"\\n\\n\")\n",
    "        if current_tokens + table_length > max_tokens:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            current_chunk = table + \"\\n\\n\"\n",
    "            current_tokens = table_length\n",
    "        else:\n",
    "            current_chunk += table + \"\\n\\n\"\n",
    "            current_tokens += table_length\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    logging.info(f\"Successfully split into {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n",
    "# Construct prompt for analysis to send to DeepSeek API\n",
    "def construct_prompt_for_analysis(chunk: str) -> str:\n",
    "    \"\"\"\n",
    "    Construct the prompt to send to the DeepSeek API for analysis.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a data architecture and data quality expert. Based on the following database table structures and sample data, help analyze the relationships between tables and between fields, and provide data quality control and data analysis recommendations.\n",
    "\n",
    "**Database Table Structures and Sample Data:**\n",
    "{chunk}\n",
    "\n",
    "Please answer the following questions based on the above information:\n",
    "1. Relationships between tables (e.g., foreign keys, associations).\n",
    "2. The role and importance of each field.\n",
    "3. Data quality control recommendations (e.g., field integrity, data consistency).\n",
    "4. Data analysis recommendations (e.g., possible analysis directions, key metrics).\n",
    "\n",
    "Please return only the analysis results in JSON format without any additional content.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# Process analysis by calling DeepSeek API for each text chunk\n",
    "def process_analysis(config: Config, chunks: List[str]):\n",
    "    \"\"\"\n",
    "    Process each text chunk by calling the DeepSeek API for analysis and save the results.\n",
    "    Implements resume functionality.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting to call DeepSeek API for analysis...\")\n",
    "    analysis_results = []\n",
    "    processed_chunks = 0\n",
    "\n",
    "    # Check if analysis results already exist\n",
    "    if os.path.exists(config.OUTPUT_ANALYSIS_FILE):\n",
    "        try:\n",
    "            with open(config.OUTPUT_ANALYSIS_FILE, 'r', encoding='utf-8') as f:\n",
    "                existing_results = json.load(f)\n",
    "            logging.info(f\"Loaded {len(existing_results)} existing analysis results.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading existing analysis results: {e}\")\n",
    "            existing_results = []\n",
    "    else:\n",
    "        existing_results = []\n",
    "\n",
    "    for idx, chunk in enumerate(tqdm(chunks, desc=\"Analyzing Chunks\")):\n",
    "        # Check if the chunk has already been processed\n",
    "        if idx < len(existing_results):\n",
    "            logging.info(f\"Chunk {idx + 1} already exists, skipping.\")\n",
    "            continue\n",
    "\n",
    "        prompt = construct_prompt_for_analysis(chunk)\n",
    "        start_time = time.time()\n",
    "        analysis = call_deepseek_api(prompt, config)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logging.info(f\"API call for chunk {idx + 1} took {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "        if analysis:\n",
    "            analysis_json = extract_json_response(analysis)\n",
    "            if analysis_json:\n",
    "                analysis_results.append(analysis_json)\n",
    "                logging.info(f\"Successfully retrieved analysis result for chunk {idx + 1}.\")\n",
    "            else:\n",
    "                analysis_results.append({\"error\": \"Unable to parse JSON from API response.\", \"raw_response\": analysis})\n",
    "                logging.warning(f\"Analysis result for chunk {idx + 1} could not be parsed.\")\n",
    "        else:\n",
    "            analysis_results.append({\"error\": \"No valid API response received.\", \"raw_response\": \"\"})\n",
    "            logging.warning(f\"Analysis result for chunk {idx + 1} is empty.\")\n",
    "\n",
    "        processed_chunks += 1\n",
    "\n",
    "    # Save all analysis results\n",
    "    try:\n",
    "        # Merge existing results with new results\n",
    "        combined_results = existing_results + analysis_results\n",
    "        with open(config.OUTPUT_ANALYSIS_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(combined_results, f, ensure_ascii=False, indent=4)\n",
    "        logging.info(f\"All analysis results have been saved to '{config.OUTPUT_ANALYSIS_FILE}'.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving final analysis results: {e}\")\n",
    "\n",
    "# Integrate merged table information into a large file (optional)\n",
    "def integrate_merged_tables(merged_clusters: List[List[str]], table_structures: Dict[str, list], table_samples: Dict[str, list], config: Config):\n",
    "    \"\"\"\n",
    "    Integrate all merged table information into a single large file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(config.GLOBAL_CONTEXT_FILE, 'r', encoding='utf-8') as f:\n",
    "            global_context = f.read()\n",
    "        logging.info(f\"Successfully loaded global context file '{config.GLOBAL_CONTEXT_FILE}'.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading global context file '{config.GLOBAL_CONTEXT_FILE}': {e}\")\n",
    "        global_context = \"\"\n",
    "    \n",
    "    combined_text = global_context + \"\\n\\n\"\n",
    "    \n",
    "    for merged_cluster in merged_clusters:\n",
    "        if not merged_cluster:\n",
    "            continue\n",
    "        if len(merged_cluster) == 1:\n",
    "            table = merged_cluster[0]\n",
    "            merged_table_name = table\n",
    "            structure = table_structures.get(table, [])\n",
    "            samples = table_samples.get(table, [])\n",
    "        else:\n",
    "            merged_table_name = '|'.join(sorted(merged_cluster))\n",
    "            structure = {}\n",
    "            samples = []\n",
    "            for table in merged_cluster:\n",
    "                for column in table_structures.get(table, []):\n",
    "                    col_name = column['name']\n",
    "                    if col_name not in structure:\n",
    "                        structure[col_name] = column\n",
    "                    else:\n",
    "                        if structure[col_name]['type'] != column['type']:\n",
    "                            logging.warning(f\"Field type mismatch: {col_name} in table '{table}' has type {column['type']}, already exists with type {structure[col_name]['type']}\")\n",
    "                            # Choose to keep the first type or handle accordingly\n",
    "            # Merge sample data\n",
    "            for table in merged_cluster:\n",
    "                samples.extend(table_samples.get(table, []))\n",
    "        \n",
    "        combined_text += f\"\\nMerged Table Name: {merged_table_name}\\n\"\n",
    "        combined_text += \"Table Structure:\\n\"\n",
    "        for column_name, column_info in structure.items():\n",
    "            combined_text += f\"  - {column_info['name']} ({column_info['type']})\"\n",
    "            if not column_info['nullable']:\n",
    "                combined_text += \" [NOT NULL]\"\n",
    "            if column_info['default']:\n",
    "                combined_text += f\" [DEFAULT {column_info['default']}]\"\n",
    "            combined_text += \"\\n\"\n",
    "        \n",
    "        # Construct sample data description\n",
    "        combined_text += \"Sample Data:\\n\"\n",
    "        for record in samples:\n",
    "            record_str = ', '.join([f\"{k}: {v}\" for k, v in record.items()])\n",
    "            combined_text += f\"  - {{ {record_str} }}\\n\"\n",
    "        combined_text += \"\\n\"\n",
    "    \n",
    "    # Save the integrated large text\n",
    "    try:\n",
    "        with open(config.COMBINED_OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "            f.write(combined_text)\n",
    "        logging.info(f\"Successfully integrated and saved merged table information to '{config.COMBINED_OUTPUT_FILE}'.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving integrated large text: {e}\")\n",
    "\n",
    "# Extract structured features from table structures\n",
    "def extract_structured_features(table_structures: Dict[str, list]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract structured features such as number of columns, number of primary keys, number of foreign keys, etc.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for table, structure in table_structures.items():\n",
    "        num_columns = len(structure)\n",
    "        num_primary_keys = sum(1 for col in structure if 'PRIMARY KEY' in col['type'].upper())\n",
    "        num_foreign_keys = sum(1 for col in structure if 'FOREIGN KEY' in col['type'].upper())\n",
    "        features.append({\n",
    "            'table_name': table,\n",
    "            'num_columns': num_columns,\n",
    "            'num_primary_keys': num_primary_keys,\n",
    "            'num_foreign_keys': num_foreign_keys\n",
    "        })\n",
    "    df_features = pd.DataFrame(features).set_index('table_name')\n",
    "    return df_features\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    config = Config()\n",
    "\n",
    "    # Check for API key\n",
    "    if not config.DEEPSEEK_API_KEY:\n",
    "        logging.error(\"DeepSeek API key not found in environment variables.\")\n",
    "        return\n",
    "\n",
    "    # Create output directory\n",
    "    create_output_directory(config.OUTPUT_DIR)\n",
    "\n",
    "    # Connect to the database\n",
    "    try:\n",
    "        engine = get_database_engine(config.DATABASE)\n",
    "    except Exception:\n",
    "        logging.error(\"Database connection failed, terminating script.\")\n",
    "        return\n",
    "\n",
    "    # Get all table names\n",
    "    tables = get_all_tables(engine, config.SCHEMA)\n",
    "\n",
    "    inspector = inspect(engine)\n",
    "\n",
    "    # Iterate over all tables, get structure and sample data, and save as JSON files\n",
    "    for table in tqdm(tables, desc=\"Processing Tables\"):\n",
    "        structure = get_table_structure(inspector, table, config.SCHEMA)\n",
    "        samples = get_sample_data(engine, table, config.SCHEMA, config.SAMPLE_SIZE)\n",
    "        if not structure:\n",
    "            logging.warning(f\"Table '{table}' has empty structure information.\")\n",
    "        if not samples:\n",
    "            logging.warning(f\"Table '{table}' has empty sample data.\")\n",
    "        save_table_info(config.OUTPUT_DIR, table, structure, samples)\n",
    "\n",
    "    # Generate table descriptions\n",
    "    descriptions = get_all_table_descriptions(config)\n",
    "\n",
    "    # Prepare dictionaries for table structures and sample data\n",
    "    table_structures = {}\n",
    "    table_samples = {}\n",
    "    for table in descriptions:\n",
    "        file_path = os.path.join(config.OUTPUT_DIR, f\"{table}.json\")\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            table_structures[table] = data.get('structure', [])\n",
    "            table_samples[table] = data.get('sample_data', [])\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.error(f\"JSON parsing error in file '{file_path}': {e}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error reading file '{file_path}': {e}\")\n",
    "            continue\n",
    "\n",
    "    # Extract structured features\n",
    "    structured_features = extract_structured_features(table_structures)\n",
    "\n",
    "    # Initialize evaluation results list\n",
    "    evaluation_results = []\n",
    "\n",
    "    # Iterate over clustering methods\n",
    "    for clustering_method, method_info in config.CLUSTERING_METHODS.items():\n",
    "        for clustering_param in method_info['params']:\n",
    "            for feature_method in config.FEATURE_EXTRACTION_METHODS:\n",
    "                logging.info(f\"Clustering method: {clustering_method}, Parameter: {clustering_param}, Feature extraction method: {feature_method}\")\n",
    "\n",
    "                # Generate feature vectors based on the feature extraction method\n",
    "                if feature_method == 'tfidf':\n",
    "                    # Generate TF-IDF features\n",
    "                    tfidf_matrix, table_names = generate_tfidf_features(descriptions)\n",
    "                    features_matrix = tfidf_matrix\n",
    "                    logging.info(\"Generated feature vectors using TF-IDF.\")\n",
    "                elif feature_method == 'sentence_transformer':\n",
    "                    # Generate SentenceTransformer embeddings and standardize\n",
    "                    embeddings, table_names = generate_sentence_embeddings(descriptions)\n",
    "                    scaler = StandardScaler()\n",
    "                    embeddings_scaled = scaler.fit_transform(embeddings)\n",
    "                    features_matrix = embeddings_scaled\n",
    "                    logging.info(\"Generated feature vectors using SentenceTransformer and standardized them.\")\n",
    "                else:\n",
    "                    logging.error(f\"Unknown feature extraction method: {feature_method}\")\n",
    "                    continue\n",
    "\n",
    "                # Combine with structured features\n",
    "                try:\n",
    "                    combined_features = np.hstack((features_matrix, structured_features.loc[table_names].values))\n",
    "                except KeyError as e:\n",
    "                    logging.error(f\"Table name mismatch: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Decide whether to perform PCA dimensionality reduction based on configuration\n",
    "                if config.ENABLE_PCA:\n",
    "                    reduced_features = reduce_dimensionality(combined_features, n_components=100)\n",
    "                    features_to_use = reduced_features\n",
    "                    logging.info(\"Enabled PCA dimensionality reduction.\")\n",
    "                else:\n",
    "                    features_to_use = combined_features\n",
    "                    logging.info(\"PCA dimensionality reduction not enabled.\")\n",
    "\n",
    "                # Perform clustering based on the method\n",
    "                if clustering_method.startswith('kmeans'):\n",
    "                    if clustering_method == 'kmeans_manual':\n",
    "                        clusters = cluster_tables_kmeans(features_to_use, clustering_param, table_names)\n",
    "                        logging.info(f\"Performed K-Means clustering with K={clustering_param}, resulting in {len(clusters)} clusters.\")\n",
    "                    elif clustering_method == 'kmeans_auto':\n",
    "                        clusters = cluster_tables_kmeans(features_to_use, None, table_names)  # None indicates automatic detection\n",
    "                        logging.info(f\"Performed K-Means clustering with automatic cluster count, resulting in {len(clusters)} clusters.\")\n",
    "                    else:\n",
    "                        logging.error(f\"Unknown K-Means clustering method: {clustering_method}\")\n",
    "                        continue\n",
    "                elif clustering_method == 'hierarchical':\n",
    "                    clusters = cluster_tables_hierarchical(features_to_use, clustering_param, table_names)\n",
    "                    logging.info(f\"Performed Hierarchical clustering with distance threshold {clustering_param}, resulting in {len(clusters)} clusters.\")\n",
    "                elif clustering_method.startswith('dbscan'):\n",
    "                    if clustering_method == 'dbscan_manual':\n",
    "                        eps, min_samples = clustering_param\n",
    "                        clusters = cluster_tables_dbscan(features_to_use, eps, min_samples, table_names, config)\n",
    "                        logging.info(f\"Performed DBSCAN clustering with ε={eps}, min_samples={min_samples}, resulting in {len(clusters)} clusters.\")\n",
    "                    elif clustering_method == 'dbscan_auto':\n",
    "                        clusters = cluster_tables_dbscan(features_to_use, None, 5, table_names, config)  # Automatically select eps, min_samples=5\n",
    "                        logging.info(f\"Performed DBSCAN clustering with automatic parameter selection, resulting in {len(clusters)} clusters.\")\n",
    "                    else:\n",
    "                        logging.error(f\"Unknown DBSCAN clustering method: {clustering_method}\")\n",
    "                        continue\n",
    "                else:\n",
    "                    logging.error(f\"Unknown clustering method: {clustering_method}\")\n",
    "                    continue\n",
    "\n",
    "                # Record results from clustering only\n",
    "                ai_grouping = {}\n",
    "                for idx, cluster in enumerate(clusters):\n",
    "                    for table in cluster:\n",
    "                        ai_grouping[table] = idx  # Use cluster index as group ID\n",
    "\n",
    "                # Load or generate manual grouping results\n",
    "                manual_grouping = load_manual_grouping(config.MANUAL_GROUPING_FILE, ai_grouping)\n",
    "                if not manual_grouping:\n",
    "                    logging.error(\"Manual grouping results are empty, cannot perform evaluation.\")\n",
    "                    continue\n",
    "\n",
    "                # Evaluate clustering-only results\n",
    "                metrics = evaluate_groupings(manual_grouping, ai_grouping)\n",
    "                logging.info(f\"Evaluation Metrics (Clustering Only): {metrics}\")\n",
    "\n",
    "                # Record evaluation results (clustering only)\n",
    "                evaluation_results.append({\n",
    "                    'experiment_type': 'clustering_only',\n",
    "                    'clustering_method': clustering_method,\n",
    "                    'clustering_param': clustering_param,\n",
    "                    'feature_extraction_method': feature_method,\n",
    "                    'similarity_threshold': None,\n",
    "                    'num_clusters': len(clusters),\n",
    "                    'num_pairs_to_compute': 0,  # No similarity computation for clustering-only\n",
    "                    'adjusted_rand_index': metrics['adjusted_rand_index'],\n",
    "                    'normalized_mutual_info': metrics['normalized_mutual_info'],\n",
    "                    'precision': metrics['precision'],\n",
    "                    'recall': metrics['recall'],\n",
    "                    'f1_score': metrics['f1_score']\n",
    "                })\n",
    "\n",
    "                # Save clustering-only AI grouping results to CSV\n",
    "                save_groupings_to_csv(ai_grouping, config, None, clustering_method, clustering_param)\n",
    "\n",
    "                # If similarity calculation and merging is enabled\n",
    "                if config.ENABLE_SIMILARITY_CALCULATION:\n",
    "                    for similarity_threshold in config.SIMILARITY_THRESHOLDS:\n",
    "                        logging.info(f\"Performing Clustering + API Matching with similarity threshold: {similarity_threshold}\")\n",
    "\n",
    "                        # Compute similarity within clusters and merge based on similarity threshold\n",
    "                        similarity_scores, num_pairs_to_compute = compute_similarity_within_clusters(\n",
    "                            descriptions, config, clusters, max_workers=3\n",
    "                        )\n",
    "                        # Merge tables based on similarity threshold\n",
    "                        merged_clusters = merge_tables_by_similarity(similarity_scores, similarity_threshold, table_names)\n",
    "                        logging.info(f\"After merging with similarity threshold {similarity_threshold}, there are {len(merged_clusters)} table groups.\")\n",
    "\n",
    "                        # Generate AI grouping results after merging\n",
    "                        ai_grouping_api = {}\n",
    "                        for idx, cluster in enumerate(merged_clusters):\n",
    "                            for table in cluster:\n",
    "                                ai_grouping_api[table] = idx  # Use cluster index as group ID\n",
    "\n",
    "                        # Evaluate clustering + API matching results\n",
    "                        metrics_api = evaluate_groupings(manual_grouping, ai_grouping_api)\n",
    "                        logging.info(f\"Evaluation Metrics (Clustering + API): {metrics_api}\")\n",
    "\n",
    "                        # Record evaluation results (clustering + API)\n",
    "                        evaluation_results.append({\n",
    "                            'experiment_type': 'clustering + API',\n",
    "                            'clustering_method': clustering_method,\n",
    "                            'clustering_param': clustering_param,\n",
    "                            'feature_extraction_method': feature_method,\n",
    "                            'similarity_threshold': similarity_threshold,\n",
    "                            'num_clusters': len(merged_clusters),\n",
    "                            'num_pairs_to_compute': num_pairs_to_compute,\n",
    "                            'adjusted_rand_index': metrics_api['adjusted_rand_index'],\n",
    "                            'normalized_mutual_info': metrics_api['normalized_mutual_info'],\n",
    "                            'precision': metrics_api['precision'],\n",
    "                            'recall': metrics_api['recall'],\n",
    "                            'f1_score': metrics_api['f1_score']\n",
    "                        })\n",
    "\n",
    "                        # Save clustering + API grouping results to CSV\n",
    "                        save_groupings_to_csv(ai_grouping_api, config, similarity_threshold, clustering_method, clustering_param)\n",
    "\n",
    "    # Convert evaluation results list to DataFrame\n",
    "    evaluation_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "    # Save evaluation results to CSV\n",
    "    try:\n",
    "        evaluation_df.to_csv(config.EVALUATION_RESULTS_FILE, index=False, encoding='utf-8-sig')\n",
    "        logging.info(f\"All evaluation results have been saved to '{config.EVALUATION_RESULTS_FILE}'.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving evaluation results to CSV: {e}\")\n",
    "\n",
    "    # Integrate merged table information (optional)\n",
    "    # Enable as needed\n",
    "    # integrate_merged_tables(merged_clusters_final, table_structures, table_samples, config)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Token Length Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 13:04:37,633 - INFO - Created post-merged directory: maude_schema_merged\n",
      "2024-11-02 13:04:37,639 - INFO - Loading 113 JSON files from 'maude_schema_analysis'.\n",
      "Loading JSON files: 100%|██████████| 113/113 [00:00<00:00, 2439.38it/s]\n",
      "2024-11-02 13:04:37,688 - INFO - Successfully loaded data for 113 tables.\n",
      "2024-11-02 13:04:37,695 - INFO - Generated descriptions for 113 tables.\n",
      "2024-11-02 13:04:37,730 - INFO - TF-IDF feature extraction completed.\n",
      "2024-11-02 13:04:37,756 - INFO - K-Means clustering with k=3 completed.\n",
      "2024-11-02 13:04:37,769 - INFO - Silhouette Score: 0.1344\n",
      "2024-11-02 13:04:37,773 - INFO - Successfully loaded similarity cache file 'similarity_cache.json'.\n",
      "2024-11-02 13:04:37,774 - INFO - Initial number of clusters formed: 3.\n",
      "2024-11-02 13:04:37,779 - INFO - Total number of merged clusters after applying similarity threshold 0.7: 14.\n",
      "2024-11-02 13:04:37,780 - INFO - Output directory for merged data already exists: maude_schema_merged\n",
      "2024-11-02 13:04:37,782 - INFO - Merged table saved: maude_schema_merged/Merged_Table_1.json\n",
      "2024-11-02 13:04:37,783 - INFO - Merged table saved: maude_schema_merged/Merged_Table_2.json\n",
      "2024-11-02 13:04:37,785 - INFO - Merged table saved: maude_schema_merged/Merged_Table_3.json\n",
      "2024-11-02 13:04:37,787 - INFO - Merged table saved: maude_schema_merged/Merged_Table_4.json\n",
      "2024-11-02 13:04:37,789 - INFO - Merged table saved: maude_schema_merged/Merged_Table_5.json\n",
      "2024-11-02 13:04:37,791 - INFO - Merged table saved: maude_schema_merged/Merged_Table_6.json\n",
      "2024-11-02 13:04:37,794 - INFO - Merged table saved: maude_schema_merged/Merged_Table_7.json\n",
      "2024-11-02 13:04:37,798 - INFO - Merged table saved: maude_schema_merged/Merged_Table_8.json\n",
      "2024-11-02 13:04:37,800 - INFO - Merged table saved: maude_schema_merged/Merged_Table_9.json\n",
      "2024-11-02 13:04:37,801 - INFO - Merged table saved: maude_schema_merged/Merged_Table_10.json\n",
      "2024-11-02 13:04:37,803 - INFO - Merged table saved: maude_schema_merged/Merged_Table_11.json\n",
      "2024-11-02 13:04:37,804 - INFO - Merged table saved: maude_schema_merged/Merged_Table_12.json\n",
      "2024-11-02 13:04:37,805 - INFO - Merged table saved: maude_schema_merged/Merged_Table_13.json\n",
      "2024-11-02 13:04:37,807 - INFO - Merged table saved: maude_schema_merged/Merged_Table_14.json\n",
      "2024-11-02 13:04:37,808 - INFO - Counting tokens in 113 pre-merged JSON files.\n",
      "Counting tokens before merging: 100%|██████████| 113/113 [00:00<00:00, 550.18it/s]\n",
      "2024-11-02 13:04:38,018 - INFO - Counting tokens in 14 post-merged JSON files.\n",
      "Counting tokens after merging: 100%|██████████| 14/14 [00:00<00:00, 590.40it/s]\n",
      "2024-11-02 13:04:38,045 - INFO - Total tokens before merging: 240095\n",
      "2024-11-02 13:04:38,045 - INFO - Total tokens after merging: 27421\n",
      "2024-11-02 13:04:38,047 - INFO - Loaded context from 'context.txt'.\n",
      "2024-11-02 13:04:38,054 - INFO - Prompt file generated and saved to 'prompt_before_merging.txt'.\n",
      "2024-11-02 13:04:38,055 - INFO - Loading 14 JSON files from 'maude_schema_merged'.\n",
      "Loading JSON files: 100%|██████████| 14/14 [00:00<00:00, 2781.11it/s]\n",
      "2024-11-02 13:04:38,063 - INFO - Successfully loaded data for 14 tables.\n",
      "2024-11-02 13:04:38,064 - INFO - Generated descriptions for 14 tables.\n",
      "2024-11-02 13:04:38,066 - INFO - Prompt file generated and saved to 'prompt_after_merging.txt'.\n",
      "2024-11-02 13:04:38,137 - INFO - Total tokens in 'prompt_before_merging.txt': 113954\n",
      "2024-11-02 13:04:38,147 - INFO - Total tokens in 'prompt_after_merging.txt': 15831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Token Count Comparison ===\n",
      "Total tokens in context.txt: 2167\n",
      "Total tokens before merging (including context): 113954\n",
      "Total tokens after merging (including context): 15831\n",
      "Tokens from table descriptions before merging: 111787\n",
      "Tokens from table descriptions after merging: 13664\n",
      "Token reduction in table descriptions: 98123 tokens (87.78%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import networkx as nx\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration class\n",
    "class Config:\n",
    "    # Directories and file paths\n",
    "    PRE_MERGED_DIR = os.getenv('PRE_MERGED_DIR', 'maude_schema_analysis')\n",
    "    POST_MERGED_DIR = os.getenv('POST_MERGED_DIR', 'maude_schema_merged')\n",
    "    SIMILARITY_CACHE_FILE = os.getenv('SIMILARITY_CACHE_FILE', 'similarity_cache.json')\n",
    "    \n",
    "    # DeepSeek API configuration (not used in this script but kept for future use)\n",
    "    DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')\n",
    "    DEEPSEEK_BASE_URL = os.getenv('DEEPSEEK_BASE_URL', 'https://api.deepseek.com/v1')\n",
    "    \n",
    "    # Clustering and feature extraction parameters\n",
    "    SIMILARITY_THRESHOLD = float(os.getenv('SIMILARITY_THRESHOLD', 0.8))\n",
    "    CLUSTERING_METHOD = os.getenv('CLUSTERING_METHOD', 'kmeans_manual')  # 'kmeans_manual'\n",
    "    KMEANS_CLUSTERS = int(os.getenv('KMEANS_CLUSTERS', 3))\n",
    "    FEATURE_EXTRACTION_METHOD = os.getenv('FEATURE_EXTRACTION_METHOD', 'tfidf')  # 'tfidf' or 'sentence_transformer'\n",
    "    \n",
    "    # Random seed and encoding\n",
    "    RANDOM_SEED = int(os.getenv('RANDOM_SEED', 42))\n",
    "    TOKEN_ENCODING = os.getenv('TOKEN_ENCODING', 'gpt2')  # Compatible with OpenAI's GPT models\n",
    "    \n",
    "    # Context and prompt files\n",
    "    CONTEXT_FILE = os.getenv('CONTEXT_FILE', 'context.txt')\n",
    "    PROMPT_BEFORE_FILE = os.getenv('PROMPT_BEFORE_FILE', 'prompt_before_merging.txt')\n",
    "    PROMPT_AFTER_FILE = os.getenv('PROMPT_AFTER_FILE', 'prompt_after_merging.txt')\n",
    "    \n",
    "    # Check for necessary environment variables\n",
    "    if not DEEPSEEK_API_KEY:\n",
    "        logging.warning(\"DEEPSEEK_API_KEY not found in environment variables. API calls will not be performed.\")\n",
    "        # Uncomment the following line if API calls are required\n",
    "        # raise ValueError(\"DEEPSEEK_API_KEY not set.\")\n",
    "    \n",
    "    # Create post-merged directory if it doesn't exist\n",
    "    if not os.path.exists(POST_MERGED_DIR):\n",
    "        os.makedirs(POST_MERGED_DIR)\n",
    "        logging.info(f\"Created post-merged directory: {POST_MERGED_DIR}\")\n",
    "    else:\n",
    "        logging.info(f\"Post-merged directory already exists: {POST_MERGED_DIR}\")\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(config.RANDOM_SEED)\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "\n",
    "def count_tokens(text: str, encoding_name: str = \"gpt2\") -> int:\n",
    "    \"\"\"\n",
    "    Count the number of tokens in the text using the tiktoken library.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.get_encoding(encoding_name)\n",
    "        tokens = encoding.encode(text)\n",
    "        return len(tokens)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error counting tokens: {e}\")\n",
    "        return len(text)  # Fallback to character count\n",
    "\n",
    "def load_json_files(directory: str) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Load all JSON files from the specified directory.\n",
    "    Returns a dictionary with table names as keys and their data as values.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    if not os.path.isdir(directory):\n",
    "        logging.error(f\"Directory '{directory}' does not exist.\")\n",
    "        return data\n",
    "    \n",
    "    json_files = [f for f in os.listdir(directory) if f.endswith('.json')]\n",
    "    logging.info(f\"Loading {len(json_files)} JSON files from '{directory}'.\")\n",
    "    \n",
    "    for json_file in tqdm(json_files, desc=\"Loading JSON files\"):\n",
    "        file_path = os.path.join(directory, json_file)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                table_data = json.load(f)\n",
    "            table_name = table_data.get('table_name')\n",
    "            if table_name:\n",
    "                data[table_name] = table_data\n",
    "            else:\n",
    "                logging.warning(f\"'table_name' not found in '{json_file}'. Skipping.\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.error(f\"JSON decoding error in file '{json_file}': {e}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error reading file '{json_file}': {e}\")\n",
    "    \n",
    "    logging.info(f\"Successfully loaded data for {len(data)} tables.\")\n",
    "    return data\n",
    "\n",
    "def generate_table_descriptions(data: Dict[str, Dict]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Generate textual descriptions for each table, including structure and sample data.\n",
    "    Returns a dictionary with table names as keys and descriptions as values.\n",
    "    \"\"\"\n",
    "    descriptions = {}\n",
    "    for table, content in data.items():\n",
    "        structure = content.get('structure', [])\n",
    "        sample_data = content.get('sample_data', [])\n",
    "        \n",
    "        # Create a textual representation of the structure\n",
    "        structure_text = \"Fields: \" + \", \".join([f\"{col['name']} ({col['type']})\" for col in structure])\n",
    "        \n",
    "        # Create a textual summary of sample data\n",
    "        sample_texts = []\n",
    "        for sample in sample_data[:3]:  # Take up to first 3 samples\n",
    "            sample_str = \", \".join([f\"{k}: {v}\" for k, v in sample.items()])\n",
    "            sample_texts.append(f\"{{ {sample_str} }}\")\n",
    "        samples_text = \"Samples: \" + \"; \".join(sample_texts)\n",
    "        \n",
    "        # Combine into a description\n",
    "        description = f\"Table '{table}': {structure_text}. {samples_text}.\"\n",
    "        descriptions[table] = description\n",
    "    \n",
    "    logging.info(f\"Generated descriptions for {len(descriptions)} tables.\")\n",
    "    return descriptions\n",
    "\n",
    "def extract_features(descriptions: Dict[str, str], method: str = \"tfidf\") -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Extract feature vectors from table descriptions using the specified method.\n",
    "    Returns the feature matrix and the list of table names.\n",
    "    \"\"\"\n",
    "    table_names = list(descriptions.keys())\n",
    "    corpus = [descriptions[table] for table in table_names]\n",
    "    \n",
    "    if method == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "        feature_matrix = vectorizer.fit_transform(corpus).toarray()\n",
    "        logging.info(\"TF-IDF feature extraction completed.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported feature extraction method: {method}\")\n",
    "    \n",
    "    return feature_matrix, table_names\n",
    "\n",
    "def perform_kmeans_clustering(features: np.ndarray, k: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Perform K-Means clustering on the feature matrix.\n",
    "    Returns the list of cluster labels.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=k, random_state=config.RANDOM_SEED)\n",
    "    labels = kmeans.fit_predict(features)\n",
    "    logging.info(f\"K-Means clustering with k={k} completed.\")\n",
    "    return labels\n",
    "\n",
    "def compute_silhouette(features: np.ndarray, labels: List[int]) -> float:\n",
    "    \"\"\"\n",
    "    Compute the silhouette score for the clustering.\n",
    "    \"\"\"\n",
    "    if len(set(labels)) > 1:\n",
    "        from sklearn.metrics import silhouette_score\n",
    "        score = silhouette_score(features, labels)\n",
    "        logging.info(f\"Silhouette Score: {score:.4f}\")\n",
    "        return score\n",
    "    else:\n",
    "        logging.warning(\"Silhouette Score cannot be computed with only one cluster.\")\n",
    "        return 0.0\n",
    "\n",
    "def load_similarity_cache(cache_file: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Load the similarity cache.\n",
    "    Returns a dictionary with \"table1|table2\" as keys and similarity scores as values.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(cache_file):\n",
    "        logging.error(f\"Similarity cache file '{cache_file}' does not exist.\")\n",
    "        return {}\n",
    "    try:\n",
    "        with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "            cache = json.load(f)\n",
    "        logging.info(f\"Successfully loaded similarity cache file '{cache_file}'.\")\n",
    "        return cache\n",
    "    except json.JSONDecodeError as e:\n",
    "        logging.error(f\"JSON decoding error in similarity cache file '{cache_file}': {e}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading similarity cache file '{cache_file}': {e}\")\n",
    "        return {}\n",
    "\n",
    "def merge_clusters(labels: List[int], table_names: List[str], similarity_threshold: float, descriptions: Dict[str, str], similarity_cache: Dict[str, float]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Merge clusters based on similarity threshold.\n",
    "    Returns a list of merged clusters, each cluster is a list of table names.\n",
    "    \"\"\"\n",
    "    # Group tables by their cluster labels\n",
    "    clusters = {}\n",
    "    for label, table in zip(labels, table_names):\n",
    "        clusters.setdefault(label, []).append(table)\n",
    "    \n",
    "    logging.info(f\"Initial number of clusters formed: {len(clusters)}.\")\n",
    "    \n",
    "    merged_clusters = []\n",
    "    \n",
    "    for cluster_tables in clusters.values():\n",
    "        if len(cluster_tables) == 1:\n",
    "            merged_clusters.append(cluster_tables)\n",
    "            continue\n",
    "        \n",
    "        # Create a graph where edges represent similarity >= threshold\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(cluster_tables)\n",
    "        \n",
    "        # Compare each pair within the cluster\n",
    "        for i in range(len(cluster_tables)):\n",
    "            for j in range(i + 1, len(cluster_tables)):\n",
    "                table1 = cluster_tables[i]\n",
    "                table2 = cluster_tables[j]\n",
    "                # Create a key in sorted order\n",
    "                key = \"|\".join(sorted([table1, table2]))\n",
    "                similarity = similarity_cache.get(key, 0.0)\n",
    "                if similarity >= similarity_threshold:\n",
    "                    G.add_edge(table1, table2)\n",
    "        \n",
    "        # Find connected components as merged clusters\n",
    "        components = list(nx.connected_components(G))\n",
    "        for component in components:\n",
    "            merged_clusters.append(list(component))\n",
    "    \n",
    "    logging.info(f\"Total number of merged clusters after applying similarity threshold {similarity_threshold}: {len(merged_clusters)}.\")\n",
    "    return merged_clusters\n",
    "\n",
    "def merge_structures_and_samples(merged_clusters: List[List[str]], data: Dict[str, Dict], output_dir: str):\n",
    "    \"\"\"\n",
    "    Merge table structures and sample data for each merged cluster.\n",
    "    Save the merged data as JSON files in the output directory.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        logging.info(f\"Created output directory for merged data: {output_dir}\")\n",
    "    else:\n",
    "        logging.info(f\"Output directory for merged data already exists: {output_dir}\")\n",
    "    \n",
    "    for idx, cluster in enumerate(merged_clusters, 1):\n",
    "        merged_table_name = f\"Merged_Table_{idx}\"\n",
    "        merged_structure = {}\n",
    "        merged_samples = []\n",
    "        \n",
    "        for table in cluster:\n",
    "            structure = data[table].get('structure', [])\n",
    "            sample_data = data[table].get('sample_data', [])\n",
    "            \n",
    "            for column in structure:\n",
    "                col_name = column['name']\n",
    "                if col_name not in merged_structure:\n",
    "                    merged_structure[col_name] = column\n",
    "                else:\n",
    "                    # If there's a type mismatch, log a warning and keep the first type\n",
    "                    if merged_structure[col_name]['type'] != column['type']:\n",
    "                        logging.warning(f\"Type mismatch for column '{col_name}' between merged table '{merged_table_name}' and table '{table}'. Keeping the first type.\")\n",
    "            \n",
    "            # Merge sample data\n",
    "            merged_samples.extend(sample_data)\n",
    "        \n",
    "        # Randomly select 3 samples\n",
    "        if len(merged_samples) > 3:\n",
    "            merged_samples = random.sample(merged_samples, 3)\n",
    "        else:\n",
    "            merged_samples = merged_samples\n",
    "        \n",
    "        # Prepare merged data\n",
    "        merged_data = {\n",
    "            'table_name': merged_table_name,\n",
    "            'structure': list(merged_structure.values()),\n",
    "            'sample_data': merged_samples\n",
    "        }\n",
    "        \n",
    "        # Save to JSON\n",
    "        output_file = os.path.join(output_dir, f\"{merged_table_name}.json\")\n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(merged_data, f, ensure_ascii=False, indent=4)\n",
    "            logging.info(f\"Merged table saved: {output_file}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving merged table '{merged_table_name}': {e}\")\n",
    "\n",
    "def generate_prompt(context_text: str, descriptions: Dict[str, str], output_file: str):\n",
    "    \"\"\"\n",
    "    Generate a prompt file by concatenating context text with table descriptions.\n",
    "    Save the result to the specified output file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        combined_text = context_text + \"\\n\\n\"\n",
    "        for table, description in descriptions.items():\n",
    "            combined_text += description + \"\\n\\n\"\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(combined_text)\n",
    "        \n",
    "        logging.info(f\"Prompt file generated and saved to '{output_file}'.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generating prompt file '{output_file}': {e}\")\n",
    "\n",
    "def count_tokens_in_file(file_path: str, encoding_name: str = \"gpt2\") -> int:\n",
    "    \"\"\"\n",
    "    Read the file and count the number of tokens.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        tokens = count_tokens(text, encoding_name)\n",
    "        logging.info(f\"Total tokens in '{file_path}': {tokens}\")\n",
    "        return tokens\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File '{file_path}' not found.\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading file '{file_path}': {e}\")\n",
    "        return 0\n",
    "\n",
    "def count_tokens_before_after(pre_merged_dir: str, post_merged_dir: str, encoding_name: str = \"gpt2\") -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Count tokens before and after merging.\n",
    "    Returns a tuple (tokens_before, tokens_after).\n",
    "    \"\"\"\n",
    "    # Count tokens before merging\n",
    "    tokens_before = 0\n",
    "    json_files = [f for f in os.listdir(pre_merged_dir) if f.endswith('.json')]\n",
    "    logging.info(f\"Counting tokens in {len(json_files)} pre-merged JSON files.\")\n",
    "    \n",
    "    for json_file in tqdm(json_files, desc=\"Counting tokens before merging\"):\n",
    "        file_path = os.path.join(pre_merged_dir, json_file)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            structure = data.get('structure', [])\n",
    "            sample_data = data.get('sample_data', [])\n",
    "            \n",
    "            # Convert structure and sample_data to text\n",
    "            structure_text = json.dumps(structure, ensure_ascii=False, indent=2)\n",
    "            sample_data_text = json.dumps(sample_data, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            # Count tokens\n",
    "            tokens_before += count_tokens(structure_text, encoding_name)\n",
    "            tokens_before += count_tokens(sample_data_text, encoding_name)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing file '{json_file}': {e}\")\n",
    "    \n",
    "    # Count tokens after merging\n",
    "    tokens_after = 0\n",
    "    merged_json_files = [f for f in os.listdir(post_merged_dir) if f.endswith('.json')]\n",
    "    logging.info(f\"Counting tokens in {len(merged_json_files)} post-merged JSON files.\")\n",
    "    \n",
    "    for json_file in tqdm(merged_json_files, desc=\"Counting tokens after merging\"):\n",
    "        file_path = os.path.join(post_merged_dir, json_file)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            structure = data.get('structure', [])\n",
    "            sample_data = data.get('sample_data', [])\n",
    "            \n",
    "            # Convert structure and sample_data to text\n",
    "            structure_text = json.dumps(structure, ensure_ascii=False, indent=2)\n",
    "            sample_data_text = json.dumps(sample_data, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            # Count tokens\n",
    "            tokens_after += count_tokens(structure_text, encoding_name)\n",
    "            tokens_after += count_tokens(sample_data_text, encoding_name)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing merged file '{json_file}': {e}\")\n",
    "    \n",
    "    logging.info(f\"Total tokens before merging: {tokens_before}\")\n",
    "    logging.info(f\"Total tokens after merging: {tokens_after}\")\n",
    "    \n",
    "    return tokens_before, tokens_after\n",
    "\n",
    "def main():\n",
    "    # Step 1: Load pre-merged data\n",
    "    data_pre = load_json_files(config.PRE_MERGED_DIR)\n",
    "    if not data_pre:\n",
    "        logging.error(\"No pre-merged data loaded. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Generate table descriptions for pre-merged data\n",
    "    descriptions_pre = generate_table_descriptions(data_pre)\n",
    "    \n",
    "    # Step 3: Extract features\n",
    "    features, table_names = extract_features(descriptions_pre, method=config.FEATURE_EXTRACTION_METHOD)\n",
    "    \n",
    "    # Step 4: Perform K-Means clustering\n",
    "    labels = perform_kmeans_clustering(features, config.KMEANS_CLUSTERS)\n",
    "    \n",
    "    # Step 5: Compute silhouette score\n",
    "    silhouette = compute_silhouette(features, labels)\n",
    "    \n",
    "    # Step 6: Load similarity cache\n",
    "    similarity_cache = load_similarity_cache(config.SIMILARITY_CACHE_FILE)\n",
    "    \n",
    "    # Step 7: Merge clusters based on similarity threshold\n",
    "    merged_clusters = merge_clusters(labels, table_names, config.SIMILARITY_THRESHOLD, descriptions_pre, similarity_cache)\n",
    "    \n",
    "    # Step 8: Merge structures and samples\n",
    "    merge_structures_and_samples(merged_clusters, data_pre, config.POST_MERGED_DIR)\n",
    "    \n",
    "    # Step 9: Count tokens before and after merging\n",
    "    tokens_before, tokens_after = count_tokens_before_after(config.PRE_MERGED_DIR, config.POST_MERGED_DIR, config.TOKEN_ENCODING)\n",
    "    \n",
    "    # Step 10: Generate prompt files\n",
    "    # Load context.txt\n",
    "    try:\n",
    "        with open(config.CONTEXT_FILE, 'r', encoding='utf-8') as f:\n",
    "            context_text = f.read()\n",
    "        logging.info(f\"Loaded context from '{config.CONTEXT_FILE}'.\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Context file '{config.CONTEXT_FILE}' not found.\")\n",
    "        context_text = \"\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading context file '{config.CONTEXT_FILE}': {e}\")\n",
    "        context_text = \"\"\n",
    "    \n",
    "    # Generate pre-merging prompt\n",
    "    generate_prompt(context_text, descriptions_pre, config.PROMPT_BEFORE_FILE)\n",
    "    \n",
    "    # Load post-merged data\n",
    "    data_post = load_json_files(config.POST_MERGED_DIR)\n",
    "    if not data_post:\n",
    "        logging.error(\"No post-merged data loaded. Skipping prompt generation for post-merging.\")\n",
    "        descriptions_post = {}\n",
    "    else:\n",
    "        # Generate table descriptions for post-merged data\n",
    "        descriptions_post = generate_table_descriptions(data_post)\n",
    "        \n",
    "        # Generate post-merging prompt\n",
    "        generate_prompt(context_text, descriptions_post, config.PROMPT_AFTER_FILE)\n",
    "    \n",
    "    # Step 11: Count tokens in prompt files\n",
    "    tokens_prompt_before = count_tokens_in_file(config.PROMPT_BEFORE_FILE, config.TOKEN_ENCODING)\n",
    "    tokens_prompt_after = count_tokens_in_file(config.PROMPT_AFTER_FILE, config.TOKEN_ENCODING)\n",
    "    \n",
    "    # Step 12: Count tokens in context.txt\n",
    "    tokens_context = count_tokens(context_text, config.TOKEN_ENCODING)\n",
    "    \n",
    "    # Step 13: Calculate tokens for table descriptions\n",
    "    tokens_table_before = tokens_prompt_before - tokens_context\n",
    "    tokens_table_after = tokens_prompt_after - tokens_context\n",
    "    \n",
    "    # Handle cases where context tokens might exceed prompt tokens\n",
    "    if tokens_table_before < 0:\n",
    "        logging.warning(\"Context tokens exceed or equal tokens in prompt_before_merging.txt. Setting table tokens before merging to 0.\")\n",
    "        tokens_table_before = 0\n",
    "    if tokens_table_after < 0:\n",
    "        logging.warning(\"Context tokens exceed or equal tokens in prompt_after_merging.txt. Setting table tokens after merging to 0.\")\n",
    "        tokens_table_after = 0\n",
    "    \n",
    "    # Step 14: Calculate reduction percentage based on table descriptions\n",
    "    if tokens_table_before == 0:\n",
    "        logging.warning(\"Token count for table descriptions before merging is zero. Cannot compute reduction percentage.\")\n",
    "        reduction_percentage = 0\n",
    "    else:\n",
    "        reduction = tokens_table_before - tokens_table_after\n",
    "        reduction_percentage = (reduction / tokens_table_before) * 100\n",
    "    \n",
    "    # Step 15: Output comparison\n",
    "    print(\"\\n=== Token Count Comparison ===\")\n",
    "    print(f\"Total tokens in context.txt: {tokens_context}\")\n",
    "    print(f\"Total tokens before merging (including context): {tokens_prompt_before}\")\n",
    "    print(f\"Total tokens after merging (including context): {tokens_prompt_after}\")\n",
    "    print(f\"Tokens from table descriptions before merging: {tokens_table_before}\")\n",
    "    print(f\"Tokens from table descriptions after merging: {tokens_table_after}\")\n",
    "    print(f\"Token reduction in table descriptions: {reduction} tokens ({reduction_percentage:.2f}%)\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reduction Ratio after Initial Clustering**\n",
    "\n",
    "The application of various clustering methods as the initialization step resulted in a substantial reduction in the number of table pairs requiring semantic similarity computations. Specifically, the initial clustering phase reduced the total number of table pair comparisons by approximately 77% to 83%, depending on the chosen clustering algorithm and its parameters. This reduction underscores the effectiveness of clustering in narrowing down potential table mergers, thereby optimizing computational resources and enhancing processing efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"evaluation_results.csv\")\n",
    "\n",
    "# Calculate the total number of possible table pairs without any clustering\n",
    "total_tables = 113\n",
    "total_possible_pairs = total_tables * (total_tables - 1) / 2\n",
    "print(f\"Total possible pairs without clustering: {total_possible_pairs}\")\n",
    "\n",
    "# Filter data for clustering only experiments\n",
    "clustering_only_data = data[data['experiment_type'] == 'clustering_only']\n",
    "print(\"Clustering only data:\")\n",
    "print(clustering_only_data)\n",
    "\n",
    "# Prepare data to analyze reduction in pairs based on clustering parameters from \"clustering + API\" experiments\n",
    "reduction_ratios = []\n",
    "\n",
    "# Iterate through clustering_only_data to assign clustering_param to relevant clustering + API rows\n",
    "for _, row in clustering_only_data.iterrows():\n",
    "    clustering_method = row['clustering_method']\n",
    "    feature_extraction_method = row['feature_extraction_method']\n",
    "    clustering_param = row['num_clusters'] if clustering_method != 'kmeans_auto' else 5\n",
    "    \n",
    "    # Get corresponding \"clustering + API\" rows with the same clustering method and feature extraction method\n",
    "    relevant_api_data = data[\n",
    "        (data['experiment_type'] == 'clustering + API') &\n",
    "        (data['clustering_method'] == clustering_method) &\n",
    "        (data['feature_extraction_method'] == feature_extraction_method) &\n",
    "        (data['clustering_param'] == row['clustering_param'])\n",
    "    ]\n",
    "    \n",
    "    # Assign clustering_param to relevant rows and calculate reduction ratios\n",
    "    for _, api_row in relevant_api_data.iterrows():\n",
    "        num_pairs_computed = api_row['num_pairs_to_compute']\n",
    "        reduction_ratio = (total_possible_pairs - num_pairs_computed) / total_possible_pairs\n",
    "        print(f\"Clustering method: {clustering_method}, Clustering param: {clustering_param}, Num pairs computed: {num_pairs_computed}, Reduction ratio: {reduction_ratio}\")\n",
    "        reduction_ratios.append({\n",
    "            'clustering_method': clustering_method,\n",
    "            'clustering_param': clustering_param,\n",
    "            'reduction_ratio': reduction_ratio\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame for easy plotting\n",
    "reduction_data = pd.DataFrame(reduction_ratios)\n",
    "print(\"Reduction data:\")\n",
    "print(reduction_data)\n",
    "\n",
    "# Plotting reduction ratio with clustering parameter as x-axis\n",
    "plt.figure(figsize=(6, 4))\n",
    "unique_methods = reduction_data['clustering_method'].unique()\n",
    "markers = ['o', 's', 'D', '^', 'v', 'P']\n",
    "linestyles = ['-', '--', '-.', ':', '-', '--']\n",
    "\n",
    "sns.set_palette(\"Set2\")  # Use a simple, muted color palette\n",
    "sns.lineplot(data=reduction_data, x='clustering_param', y='reduction_ratio', hue='clustering_method', \n",
    "             style='clustering_method', markers=markers[:len(unique_methods)], dashes=True, linewidth=2, legend='full', ci=None)\n",
    "#plt.title('Reduction Ratio of Table Pairs by Clustering Method and Clustering Parameter', fontsize=18)\n",
    "plt.xlabel('Clustering Parameter', fontsize=16)\n",
    "plt.ylabel('Reduction Ratio', fontsize=16)\n",
    "plt.xticks(rotation=45, fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(title='Clustering Method', fontsize=14, title_fontsize=16, frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance Metrics with Clustering Only**\n",
    "\n",
    "When clustering was employed as the sole method for table merging without subsequent semantic similarity evaluations, the performance metrics exhibited notable variability based on the clustering technique and feature extraction method utilized. The Adjusted Rand Index (ARI) ranged from 0.31 to 0.93, the Normalized Mutual Information (NMI) ranged from 0.50 to 0.93, and the F1 score ranged from 0.51 to 0.95. These results indicate that while clustering alone can achieve moderate to high levels of agreement with manual groupings, there remains considerable variability, particularly in capturing the nuanced semantic relationships inherent in the MAUDE database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"evaluation_results.csv\")\n",
    "\n",
    "# Plotting performance metrics with clustering only data\n",
    "metrics = ['adjusted_rand_index', 'normalized_mutual_info', 'f1_score']\n",
    "metric_names = ['Adjusted Rand Index (ARI)', 'Normalized Mutual Information (NMI)', 'F1 Score']\n",
    "\n",
    "# Prepare data for plotting performance metrics\n",
    "melted_data = clustering_only_data.melt(id_vars=['clustering_method', 'feature_extraction_method'], \n",
    "                                        value_vars=metrics, \n",
    "                                        var_name='Metric', \n",
    "                                        value_name='Score')\n",
    "\n",
    "# Map metrics to descriptive names\n",
    "melted_data['Metric'] = melted_data['Metric'].replace({\n",
    "    'adjusted_rand_index': 'ARI',\n",
    "    'normalized_mutual_info': 'NMI',\n",
    "    'f1_score': 'F1 Score'\n",
    "})\n",
    "\n",
    "# Plotting the performance metrics for clustering only experiments with updated styles\n",
    "plt.figure(figsize=(8,7))\n",
    "sns.set_palette(\"pastel\")  # Use a simpler, more subdued color palette\n",
    "sns.boxplot(data=melted_data, x='Metric', y='Score', hue='clustering_method', dodge=True)\n",
    "\n",
    "#plt.title('Performance Metrics with Clustering Only by Method', fontsize=24)\n",
    "plt.xlabel('Performance Metric', fontsize=20)\n",
    "plt.ylabel('Score', fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.legend(title='Clustering Method', fontsize=16, title_fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance Metrics with Clustering + API Matching**\n",
    "\n",
    "The integration of semantic similarity evaluations using the DeepSeek V2.5 API with the initial clustering results led to remarkable improvements in clustering performance metrics. Regardless of the clustering method, feature extraction technique, or similarity threshold applied, the combined approach consistently achieved superior results. The Adjusted Rand Index (ARI) improved to a range of 0.85–1.00, Normalized Mutual Information (NMI) increased to 0.93–1.00, and the F1 score enhanced to 0.88–1.00. Notably, several configurations achieved an optimal F1 score of 1.00, reflecting perfect precision and recall. This demonstrates the efficacy of incorporating semantic similarity assessments to refine and validate the clusters generated through initial clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"evaluation_results.csv\")\n",
    "\n",
    "# Plotting performance metrics with clustering + API experiments\n",
    "# Filter data for clustering + API experiments\n",
    "clustering_api_data = data[data['experiment_type'] == 'clustering + API']\n",
    "\n",
    "# Select the performance metrics\n",
    "metrics_api = ['adjusted_rand_index', 'normalized_mutual_info', 'f1_score']\n",
    "\n",
    "# Prepare data for plotting\n",
    "melted_data_api = clustering_api_data.melt(id_vars=['clustering_method', 'feature_extraction_method'], \n",
    "                                           value_vars=metrics_api, \n",
    "                                           var_name='Metric', \n",
    "                                           value_name='Score')\n",
    "\n",
    "# Map metrics to descriptive names\n",
    "melted_data_api['Metric'] = melted_data_api['Metric'].replace({\n",
    "    'adjusted_rand_index': 'ARI',\n",
    "    'normalized_mutual_info': 'NMI',\n",
    "    'f1_score': 'F1 Score'\n",
    "})\n",
    "\n",
    "# Plotting the performance metrics for clustering + API experiments\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set_palette(\"pastel\")  # Use a simple, subdued color palette\n",
    "sns.boxplot(data=melted_data_api, x='Metric', y='Score', hue='clustering_method', dodge=True)\n",
    "\n",
    "#plt.title('Performance Metrics with Clustering + API Matching by Method', fontsize=24)\n",
    "plt.xlabel('Performance Metric', fontsize=20)\n",
    "plt.ylabel('Score', fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.legend(title='Clustering Method', fontsize=16, title_fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Impact of Feature Extraction Methods (TF-IDF vs. Sentence Transformer)**\n",
    "\n",
    "The choice of feature extraction method significantly influenced the performance of both clustering-only and clustering-plus-API matching approaches. Sentence Transformer embeddings consistently outperformed TF-IDF vectorization across all evaluation metrics.\n",
    "\n",
    "- **TF-IDF Vectorization:**\n",
    "  - *Clustering Only:* ARI ranged from 0.31 to 0.84, NMI from 0.50 to 0.88, and F1 Score from 0.51 to 0.87.\n",
    "  - *Clustering + API Matching:* ARI improved to 0.94–1.00, NMI ranged from 0.97 to 1.00, and F1 Score improved to 0.95–1.00.\n",
    "\n",
    "- **Sentence Transformer Embeddings:**\n",
    "  - *Clustering Only:* ARI ranged from 0.31 to 0.93, NMI from 0.50 to 0.93, and F1 Score from 0.51 to 0.95.\n",
    "  - *Clustering + API Matching:* ARI reached 0.85–1.00, NMI remained between 0.93 and 1.00, and F1 Score consistently achieved 0.88–1.00.\n",
    "\n",
    "These findings indicate that Sentence Transformer embeddings, which capture deeper semantic relationships and contextual nuances, provide a more robust foundation for clustering compared to the more surface-level TF-IDF vectorization. The enhanced performance metrics corroborate the superiority of contextual embeddings in capturing the intricate relationships between table descriptions inherent in the MAUDE database.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"evaluation_results.csv\")\n",
    "\n",
    "# Filter data for TF-IDF and Sentence Transformer feature extraction methods\n",
    "methods_data = data[(data['feature_extraction_method'] == 'tfidf') | (data['feature_extraction_method'] == 'sentence_transformer')]\n",
    "\n",
    "# Separate clustering only and clustering + API data\n",
    "clustering_only_data = methods_data[methods_data['experiment_type'] == 'clustering_only']\n",
    "clustering_api_data = methods_data[methods_data['experiment_type'] == 'clustering + API']\n",
    "\n",
    "# Metrics to analyze\n",
    "metrics = ['adjusted_rand_index', 'normalized_mutual_info', 'f1_score']\n",
    "\n",
    "# Prepare data for plotting performance metrics for Clustering Only\n",
    "melted_clustering_only_data = clustering_only_data.melt(id_vars=['clustering_method', 'feature_extraction_method'],\n",
    "                                                        value_vars=metrics,\n",
    "                                                        var_name='Metric',\n",
    "                                                        value_name='Score')\n",
    "\n",
    "# Map metrics to descriptive names\n",
    "melted_clustering_only_data['Metric'] = melted_clustering_only_data['Metric'].replace({\n",
    "    'adjusted_rand_index': 'ARI',\n",
    "    'normalized_mutual_info': 'NMI',\n",
    "    'f1_score': 'F1 Score'\n",
    "})\n",
    "\n",
    "# Plotting the performance metrics for Clustering Only experiments\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set_palette(\"pastel\")  # Use a simple, muted color palette\n",
    "sns.boxplot(data=melted_clustering_only_data, x='Metric', y='Score', hue='feature_extraction_method', dodge=True)\n",
    "\n",
    "#plt.title('Performance Metrics with Clustering Only by Feature Extraction Method', fontsize=24)\n",
    "plt.xlabel('Performance Metric', fontsize=20)\n",
    "plt.ylabel('Score', fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.legend(title='Feature Extraction Method', fontsize=16, title_fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prepare data for plotting performance metrics for Clustering + API\n",
    "melted_clustering_api_data = clustering_api_data.melt(id_vars=['clustering_method', 'feature_extraction_method'],\n",
    "                                                      value_vars=metrics,\n",
    "                                                      var_name='Metric',\n",
    "                                                      value_name='Score')\n",
    "\n",
    "# Map metrics to descriptive names\n",
    "melted_clustering_api_data['Metric'] = melted_clustering_api_data['Metric'].replace({\n",
    "    'adjusted_rand_index': 'ARI',\n",
    "    'normalized_mutual_info': 'NMI',\n",
    "    'f1_score': 'F1 Score'\n",
    "})\n",
    "\n",
    "# Plotting the performance metrics for Clustering + API experiments\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set_palette(\"pastel\")  # Use a simple, muted color palette\n",
    "sns.boxplot(data=melted_clustering_api_data, x='Metric', y='Score', hue='feature_extraction_method', dodge=True)\n",
    "\n",
    "#plt.title('Performance Metrics with Clustering + API by Feature Extraction Method', fontsize=24)\n",
    "plt.xlabel('Performance Metric', fontsize=20)\n",
    "plt.ylabel('Score', fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.legend(title='Feature Extraction Method', fontsize=16, title_fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HEATAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data preparation\n",
    "data_07 = {\n",
    "    \"Group ID\": list(range(13)),\n",
    "    \"Tables Count\": [21, 1, 32, 14, 4, 4, 1, 2, 1, 32, 1, 1, 1]\n",
    "}\n",
    "\n",
    "data_08 = {\n",
    "    \"Group ID\": list(range(14)),\n",
    "    \"Tables Count\": [21, 1, 32, 14, 4, 4, 1, 1, 1, 31, 1, 1, 1, 1]\n",
    "}\n",
    "\n",
    "data_09 = {\n",
    "    \"Group ID\": list(range(16)),\n",
    "    \"Tables Count\": [21, 1, 30, 14, 4, 4, 1, 1, 1, 29, 1,1, 1, 1, 3, 1]\n",
    "}\n",
    "\n",
    "# Creating DataFrames\n",
    "df_07 = pd.DataFrame(data_07)\n",
    "df_08 = pd.DataFrame(data_08)\n",
    "df_09 = pd.DataFrame(data_09)\n",
    "\n",
    "# Merging DataFrames for heatmap visualization\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Threshold 0.7\": df_07[\"Tables Count\"],\n",
    "    \"Threshold 0.8\": df_08[\"Tables Count\"],\n",
    "    \"Threshold 0.9\": df_09[\"Tables Count\"]\n",
    "})\n",
    "\n",
    "# Fill NaN values with 0 for groups that do not exist in all thresholds\n",
    "summary_df = summary_df.fillna(0)\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(summary_df, annot=True, cmap=\"YlGnBu\", cbar_kws={'label': 'Number of Tables'})\n",
    "plt.title(\"Changes in Grouping of Tables with Increasing Similarity Thresholds\")\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.ylabel(\"Group ID\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3x2 Figure Matrix in GrayScale**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"evaluation_results.csv\")\n",
    "\n",
    "# Calculate the total number of possible table pairs without any clustering\n",
    "total_tables = 113\n",
    "total_possible_pairs = total_tables * (total_tables - 1) / 2\n",
    "print(f\"Total possible pairs without clustering: {total_possible_pairs}\")\n",
    "\n",
    "# Filter data for clustering only experiments\n",
    "clustering_only_data = data[data['experiment_type'] == 'clustering_only']\n",
    "print(\"Clustering only data:\")\n",
    "print(clustering_only_data)\n",
    "\n",
    "# Prepare data to analyze reduction in pairs based on clustering parameters from \"clustering + API\" experiments\n",
    "reduction_ratios = []\n",
    "\n",
    "# Iterate through clustering_only_data to assign clustering_param to relevant clustering + API rows\n",
    "for _, row in clustering_only_data.iterrows():\n",
    "    clustering_method = row['clustering_method']\n",
    "    feature_extraction_method = row['feature_extraction_method']\n",
    "    clustering_param = row['num_clusters'] if clustering_method != 'kmeans_auto' else 5\n",
    "    \n",
    "    # Get corresponding \"clustering + API\" rows with the same clustering method and feature extraction method\n",
    "    relevant_api_data = data[\n",
    "        (data['experiment_type'] == 'clustering + API') &\n",
    "        (data['clustering_method'] == clustering_method) &\n",
    "        (data['feature_extraction_method'] == feature_extraction_method) &\n",
    "        (data['clustering_param'] == row['clustering_param'])\n",
    "    ]\n",
    "    \n",
    "    # Assign clustering_param to relevant rows and calculate reduction ratios\n",
    "    for _, api_row in relevant_api_data.iterrows():\n",
    "        num_pairs_computed = api_row['num_pairs_to_compute']\n",
    "        reduction_ratio = (total_possible_pairs - num_pairs_computed) / total_possible_pairs\n",
    "        print(f\"Clustering method: {clustering_method}, Clustering param: {clustering_param}, Num pairs computed: {num_pairs_computed}, Reduction ratio: {reduction_ratio}\")\n",
    "        reduction_ratios.append({\n",
    "            'clustering_method': clustering_method,\n",
    "            'clustering_param': clustering_param,\n",
    "            'reduction_ratio': reduction_ratio\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame for easy plotting\n",
    "reduction_data = pd.DataFrame(reduction_ratios)\n",
    "print(\"Reduction data:\")\n",
    "print(reduction_data)\n",
    "\n",
    "# Set black-and-white style for better printing\n",
    "sns.set(style=\"whitegrid\", rc={'grid.color': '0.8', 'grid.linewidth': 0.5})\n",
    "plt.style.use('grayscale')\n",
    "\n",
    "# Plotting all figures and arranging them into a 3x2 grid\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 18))\n",
    "\n",
    "# Plot 1: Reduction Ratio with clustering parameter as x-axis\n",
    "sns.lineplot(data=reduction_data, x='clustering_param', y='reduction_ratio', hue='clustering_method', style='clustering_method', markers=True, dashes=True, linewidth=2, ax=axes[2, 1], ci=None)\n",
    "axes[2, 1].set_xlabel('Clustering Parameter', fontsize=12)\n",
    "axes[2, 1].set_ylabel('Reduction Ratio', fontsize=12)\n",
    "axes[2, 1].legend(title='Clustering Method', fontsize=10, title_fontsize=12, frameon=False)\n",
    "axes[2, 1].set_title('(F) Reduction Ratio by Clustering Method', fontsize=14)\n",
    "\n",
    "# Plot 2: Performance Metrics with Clustering Only by Method\n",
    "metrics = ['adjusted_rand_index', 'normalized_mutual_info', 'f1_score']\n",
    "melted_data = clustering_only_data.melt(id_vars=['clustering_method', 'feature_extraction_method'], \n",
    "                                        value_vars=metrics, \n",
    "                                        var_name='Metric', \n",
    "                                        value_name='Score')\n",
    "melted_data['Metric'] = melted_data['Metric'].replace({\n",
    "    'adjusted_rand_index': 'ARI',\n",
    "    'normalized_mutual_info': 'NMI',\n",
    "    'f1_score': 'F1 Score'\n",
    "})\n",
    "sns.boxplot(data=melted_data, x='Metric', y='Score', hue='clustering_method', dodge=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_xlabel('Performance Metric', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Score', fontsize=12)\n",
    "axes[0, 0].legend(title='Clustering Method', fontsize=10, title_fontsize=12)\n",
    "axes[0, 0].set_title('(A) Performance Metrics (Clustering Only)', fontsize=14)\n",
    "\n",
    "# Plot 3: Performance Metrics with Clustering + API by Method\n",
    "clustering_api_data = data[data['experiment_type'] == 'clustering + API']\n",
    "melted_data_api = clustering_api_data.melt(id_vars=['clustering_method', 'feature_extraction_method'], \n",
    "                                           value_vars=metrics, \n",
    "                                           var_name='Metric', \n",
    "                                           value_name='Score')\n",
    "melted_data_api['Metric'] = melted_data_api['Metric'].replace({\n",
    "    'adjusted_rand_index': 'ARI',\n",
    "    'normalized_mutual_info': 'NMI',\n",
    "    'f1_score': 'F1 Score'\n",
    "})\n",
    "sns.boxplot(data=melted_data_api, x='Metric', y='Score', hue='clustering_method', dodge=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_xlabel('Performance Metric', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Score', fontsize=12)\n",
    "axes[0, 1].legend(title='Clustering Method', fontsize=10, title_fontsize=12)\n",
    "axes[0, 1].set_title('(B) Performance Metrics (Clustering + API)', fontsize=14)\n",
    "\n",
    "# Plot 4: Performance Metrics with Clustering Only by Feature Extraction Method\n",
    "methods_data = data[(data['feature_extraction_method'] == 'tfidf') | (data['feature_extraction_method'] == 'sentence_transformer')]\n",
    "clustering_only_data = methods_data[methods_data['experiment_type'] == 'clustering_only']\n",
    "melted_clustering_only_data = clustering_only_data.melt(id_vars=['clustering_method', 'feature_extraction_method'],\n",
    "                                                        value_vars=metrics,\n",
    "                                                        var_name='Metric',\n",
    "                                                        value_name='Score')\n",
    "melted_clustering_only_data['Metric'] = melted_clustering_only_data['Metric'].replace({\n",
    "    'adjusted_rand_index': 'ARI',\n",
    "    'normalized_mutual_info': 'NMI',\n",
    "    'f1_score': 'F1 Score'\n",
    "})\n",
    "sns.boxplot(data=melted_clustering_only_data, x='Metric', y='Score', hue='feature_extraction_method', dodge=True, ax=axes[1, 0])\n",
    "axes[1, 0].set_xlabel('Performance Metric', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Score', fontsize=12)\n",
    "axes[1, 0].legend(title='Feature Extraction Method', fontsize=10, title_fontsize=12)\n",
    "axes[1, 0].set_title('(C) Performance Metrics (Clustering Only by Feature Extraction)', fontsize=14)\n",
    "\n",
    "# Plot 5: Performance Metrics with Clustering + API by Feature Extraction Method\n",
    "clustering_api_data = methods_data[methods_data['experiment_type'] == 'clustering + API']\n",
    "melted_clustering_api_data = clustering_api_data.melt(id_vars=['clustering_method', 'feature_extraction_method'],\n",
    "                                                      value_vars=metrics,\n",
    "                                                      var_name='Metric',\n",
    "                                                      value_name='Score')\n",
    "melted_clustering_api_data['Metric'] = melted_clustering_api_data['Metric'].replace({\n",
    "    'adjusted_rand_index': 'ARI',\n",
    "    'normalized_mutual_info': 'NMI',\n",
    "    'f1_score': 'F1 Score'\n",
    "})\n",
    "sns.boxplot(data=melted_clustering_api_data, x='Metric', y='Score', hue='feature_extraction_method', dodge=True, ax=axes[1, 1])\n",
    "axes[1, 1].set_xlabel('Performance Metric', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Score', fontsize=12)\n",
    "axes[1, 1].legend(title='Feature Extraction Method', fontsize=10, title_fontsize=12)\n",
    "axes[1, 1].set_title('(D) Performance Metrics (Clustering + API by Feature Extraction)', fontsize=14)\n",
    "\n",
    "# Plot 6: Heatmap of Changes in Grouping of Tables with Increasing Similarity Thresholds\n",
    "files = [\n",
    "    \"maude_schema_analysis/groupings_kmeans_manual_sim_0.7_param_3.csv\",\n",
    "    \"maude_schema_analysis/groupings_kmeans_manual_sim_0.8_param_3.csv\",\n",
    "    \"maude_schema_analysis/groupings_kmeans_manual_sim_0.9_param_3.csv\"\n",
    "]\n",
    "\n",
    "thresholds = [\"Threshold 0.7\", \"Threshold 0.8\", \"Threshold 0.9\"]\n",
    "groupings = []\n",
    "\n",
    "data_dicts = []\n",
    "\n",
    "# Load groupings from CSV files and calculate group sizes\n",
    "dfs = []\n",
    "for file, threshold in zip(files, thresholds):\n",
    "    df = pd.read_csv(file)\n",
    "    df['Tables Count'] = df['Tables'].apply(lambda x: len(x.split(', ')))\n",
    "    dfs.append(df[['Group ID', 'Tables Count']])\n",
    "    data_dicts.append({\n",
    "        'Group ID': df['Group ID'].tolist(),\n",
    "        'Tables Count': df['Tables Count'].tolist()\n",
    "    })\n",
    "\n",
    "# Create DataFrames for each threshold\n",
    "data_07 = pd.DataFrame(data_dicts[0])\n",
    "data_08 = pd.DataFrame(data_dicts[1])\n",
    "data_09 = pd.DataFrame(data_dicts[2])\n",
    "\n",
    "# Combine data into a summary DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Threshold 0.7\": data_07.set_index('Group ID')['Tables Count'],\n",
    "    \"Threshold 0.8\": data_08.set_index('Group ID')['Tables Count'],\n",
    "    \"Threshold 0.9\": data_09.set_index('Group ID')['Tables Count']\n",
    "}).fillna(0)\n",
    "\n",
    "# Plot heatmap of group changes with cross-threshold splitting information\n",
    "sns.heatmap(summary_df, annot=True, cmap=\"Greys\", cbar_kws={'label': 'Number of Tables'}, ax=axes[2, 0], linecolor='white', linewidths=0.5)\n",
    "axes[2, 0].set_title(\"(E) Changes in Grouping of Tables with Increasing Thresholds\", fontsize=14)\n",
    "axes[2, 0].set_xlabel(\"Thresholds\", fontsize=12)\n",
    "axes[2, 0].set_ylabel(\"Group ID\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
